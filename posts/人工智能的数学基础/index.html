<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>人工智能的数学基础 | Jiajun, Shen</title>
<meta name="keywords" content="AI, Math">
<meta name="description" content="引言

经验风险$R_{emp}(f(u,x)=\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)$
期望风险(真实风险)：$R_{exp}(f(u, x)) = \mathbb{E}[l(f(u, x), v)]$
结构风险模型：$R_{srm}\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)&#43;\lambda J(f)$
全体数据集最好算法$f^$，有限样本有限算法集最佳算法$\hat{h}_H$，全体数据有限算法最佳$h_H^$
近似误差$R_{exp}(h_H^)-R^$，估算误差$R_{emp}(\hat{h}H) − R{exp}(h^∗_H)$

最优化基础


广义实值函数

基本概念

广义实值函数：映射$\mathbb{R}^n$-&gt;广义函数空间$\mathbb{R}\cup{\pm\infty}$
$\alpha$-下水平集：$C_\alpha={x|f(x)\le\alpha}$，上方图$\mathrm{epi}$ $f = { (x, t) ∈ R^{n&#43;1} |f(x) ≤ t}$
$\alpha$-下水平集是闭集&lt;=&gt;下半连续&lt;=&gt;闭函数（上方图是闭集）
对偶范数$||y||*=sup{||x||\le1}x^Ty$
梯度$\nabla f(x)=[\frac{\partial f}{\partial x_1}(x),&hellip;,\frac{\partial f}{\partial x_n}(x)]^T$，Hessian矩阵（$n\times n$）:$\nabla^2 f(x)$
方向导数$\partial f(x;d)=\frac{\partial d}{\partial d}(x)=\lim_{\theta\rightarrow 0 }\frac{f(x&#43;\theta d)-f(x)}{\theta}=\nabla f(x)^T d$
二阶方向导数$d^T\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\frac{\partial f_i}{\partial x_j}(x)$
泰勒展开式：$f(x&#43;d)=f(x)&#43;\nabla f(x&#43;td)^T d=f(x)&#43;\nabla f(x)^T d&#43;\frac{1}{2}d^T\nabla^2 f(x&#43;td)d$
凸性

凸集：$\eta x_1&#43;(1-\eta) x_2\in S$
凸函数$f(\eta x_1 &#43;(1-\eta)x_2 \le \eta f(x_1)&#43;(1-\eta)f(x_2)$&lt;=&gt;$f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)$&lt;=&gt;当且仅当在任意直线上是凸的
强凸：$\exists \mu&gt;0, f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)&#43;\frac{1}{2}\mu ||x_2-x_1||_2^2$
二阶条件$\nabla ^2 f(x)\ge 0$



利普希茨连续

存在$L$，对于任意的$x,y\in \mathrm{dom} f$有：$||\nabla f(x)-\nabla f(y)|\le L||x-y|||$&lt;=&gt;$||\nabla ^2 f(x)||\le L, \forall x$
凸函数，满足利普希茨条件，则$||\nabla f(x)-\nabla f(y)||^2\le L(x-y)^T(\nabla f(x)-\nabla f(y))$
#三个等价条件


次梯度

$f(y)\ge f(x)&#43;g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\partial f(x)$


共轭函数

$f^*(y)=sup_x{y^Tx-f(x)}$

性质：$f(x)&#43;f*(y)\ge x^Ty$，若$f$为闭函数，$f^{**}=f$









优化算法与基本结构

算法基本结构

全局最小点$f(x^)&lt;f(x)$、严格全局最小点$x^\ne x$
线搜索算法:
给定初始点x0∈R，置k:=0
若在 x[k] 点终止准则成立，则 x[k] 即为求得的最优解，终止; 否则，转步 3
根据方向计算规则，求得 x[k] 点搜索方向 d[k]
根据步长计算规则，求得搜索步长 η[k]
令x[k&#43;1]=x[k]&#43;η[k]*d[k]，置k:=k&#43;1，转步2
终止准则：$||g^k||\le \varepsilon$或$||x^{k&#43;1}-x^k||&lt;\varepsilon$或$||f(x^{k&#43;1})-f(x^k)||&lt;\varepsilon$


收敛速度

若$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||}=\beta$，$0=\beta$超线性收敛，$0&lt;\beta&lt;1$线性收敛，$\beta=1$次线性收敛
二次收敛$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||^2}=\beta$(任意常数)
存在$\alpha\ge 1,\beta &gt;0$，当$k$足够大（与$\alpha \beta$无关），恒有$||x^{k&#43;1}-x^||\le \beta ||x^k-x^||^\alpha$
如果他对于任意正定二次函数，从任意初始点出发，可以经有限步迭代求得极小点，我们就称该算法具有二次终止性





线搜索技术

精确线搜索法

Armojo准则：$d^k$是$x^k处$的下降方向，若$f(x^k&#43;\eta d^k)\le f(x^k)&#43;\rho \eta \nabla f(x^k)^T d^k$，则$\eta$满足Armijo准则
Armijo线搜索算法

选择初始步长 η，参数 ρ,γ ∈ (0,1)，初始化 η ← ηˆ
若 ηk 满足Armijo准则，则终止计算，得步长 ηk. 否则，转步
令ηk :=γηk，转步2.


Goldstein准则：在Armijo准则基础上加上$f(x^k&#43;\eta d^k)\ge f(x^k)&#43;(1-\rho) \eta \nabla f(x^k)^T d^k$


非精确线搜索

Wolfe 准则，它的核心思想有两个：目标函数值应该有足够的下降；可接受点处的切线斜率 ≥ 初始斜率的 σ 倍
在Armijo准则上加伤$\nabla f(x^k&#43;\eta d^k)^T d^k\ge \sigma \nabla f(x^k)^T d^k$
非精确线搜索步长的存在性：$f(x^k &#43; ηd^k)$ 在 $η &gt; 0$ 时有下界，且 $∇f(x^k)^Td^k &lt; 0$





最优化分支

线性与非线性规划

线性规划LP：在线性等式和不等式约束下最优化一个线性目标函数
如果约束和目标函数中有一个非线性的，则问题就称为非线性规划问题


二次规划QP

目标函数是变量的二次函数
Q半正定时QP是凸优化问题，可以用内点法在多项式时间内求解


锥优化CO

非负性条件 $x ≥ 0$ 用锥包含约束替换后得到的优化问题
二阶锥$x_1^2 ⩾ x_2^2 &#43;···&#43;x^2_n,x_1 ⩾ 0$
对称半正定锥 $X=X^T$半正定


整数规划ILP

部分或全部变量取整数的优化问题
0-1规划
混合整数规划：既有连续变量又有整数约束变量时，问题称为混合整数线性规划


动态规划

涉及递推关系的计算方法，把问题分成阶段以便进行递推优化






最优化理论

Weierstrass 定理：条件任意成立一个：$\mathrm{dom f}$有界；存在常数$\bar{gamma}$使得下水平集$C_\gamma$是非空且有界的；$f$是强制的，即对于任意满足极限为$&#43;\infty$的点列都有其函数值趋向于$&#43;\infty$，则最优化问题的最小点集是非空且紧的

无约束可微优化问题

下降方向：如果存在$d$满足$\nabla f(x)^Td&lt;0$则$d$为一个下降方向。局部最优点处不能有下降方向。局部极小点$x^$满足$\nabla f(x^)=0$(一阶必要条件)，同时$\nabla^2f(x^*)$半正定（二阶必要条件），如果二阶连续可微，那么二阶必要条件是充分条件。
假设$f$#适当 且凸，则$x^$是局部极小点&lt;=&gt;$0\in \partial f(x^)$
对于二阶连续可微的目标函数，梯度法、牛顿法、拟牛顿法在每一次迭代均能看做是构建局部的二次模型，梯度法可以看做利用 $(1/η^k)I$作为Hessian矩阵估计，牛顿类算法利用真实Hessian矩阵，拟牛顿利用真实Hessian矩阵或逆的估计构建模型。牛顿法收敛最快计算量存储量大，梯度法相对最慢。

梯度类算法

一般形式：$x^{k&#43;1}=x^k&#43;\eta_k d^k$，收敛速度：$L$-利普希茨连续时$0&lt;\eta&lt;\frac{1}{L}$时为$O(1/k)$，对强凸函数$0&lt;\eta&lt;\frac{1}{L&#43;\eta}$时Q-线性收敛
精确线搜索、数值线性搜索法
BB方法：

选取$min||\eta y^{k-1}-s^{k-1}||^2$或$min|| y^{k-1}-\eta^{-1}s^{k-1}||^2$的解
$s^{k-1}=x^{k&#43;1}-x^k$，$y^{k-1}=\nabla f(x^{k&#43;1})-\nabla f(x^k)$
解分别为$\eta_{BB1}^k=\frac{(s^{k-1})^Ty^{k-1}} {(y^{k-1})^Ty^{k-1}}$，$\eta_{BB2}^k=\frac{(s^{k-1})^Ts^{k-1}} {(s^{k-1})^Ty^{k-1}}$
通过$η_m ⩽η_k ⩽η_M$截断过大或过小的步长，也可以使用两种步长的凸组合





次梯度法

迭代格式：$x^{k&#43;1} = x^k − η^kg^k, g^k ∈ ∂f(x^k)$
若 $0 \notin ∂f(x)$，那么对于任意 $x^∗ ∈ argmin_x f(x)$和任意 $g ∈ ∂f(x)$，存在步长 $η &gt; 0$ 使得$||x−ηg−x^||_2^2 &lt;||x−x^||_2^2$
若至少存在一个极小点且次梯度有界，则$\sum \eta_k(f(x^k)-f(x^))\le \frac{1}{2}||x^0-x^||^2&#43;\frac{1}{2}\sum \eta_k^2 M^2$



经典牛顿法

迭代格式：$x^{k&#43;1} = x^k − \nabla^2f(x^k)^{-1}\nabla f(x^k), g^k ∈ ∂f(x^k)$
极小点处梯度为0，Hessian矩阵正定，则起始点足够近时，收敛是Q-二次的且梯度的范数Q-二次收敛到0



修正牛顿法

迭代格式：$x^{k&#43;1} = x^k &#43;\eta_k d^k$
确定矩阵$E^k$使得$\nabla ^2 f(x^k)&#43;E^k$正定且条件数较小，求解$B^kd^k=-\nabla f(x^k)$，确定步长迭代。



非精确牛顿法

引入残差$r^k=\nabla^2 f(x^k)d^k&#43;\nabla f(x^k)$，$||r^k||\le \alpha_k||\nabla f(x^k)||$
若存在$t&lt;1$使得$0&lt;\alpha_k&lt;t$则Q-线性收敛；若$\alpha_k$收敛到0，则Q-超线性收敛；若$\alpha_k=O(||\nabla f(x^k)||)$，则Q-二次收敛



拟牛顿条件

Hessian的近似矩阵满足$y^k=B^{k&#43;1}s^k$，逆矩阵$s^k=H^{k&#43;1}y^k$
迭代格式：$x^{k&#43;1}=x^k&#43;\alpha_k d^k$，$d^k=-(B^k)^{-1}\nabla f(x^k)=-H^k\nabla f(x^k)$
SR1秩一更新

$B^{k&#43;1}=B^k&#43;\frac{(y^k-B^ks^k)(y^k-B^ks^k)^T}{(y^k-B^ks^k)^T s^k}$
$H^{k&#43;1}=H^k&#43;\frac{(s^k-H^ky^k)(s^k-H^ky^k)^T}{(s^k-H^ky^k)^T y^k}$


秩二更新

BFGS(相当于在满足割线方程的对称矩阵中找到离 $H^k$ 最近的矩阵)

利用割线方程$Ws^k=y^k$
$B^{k&#43;1}=B^k&#43;\frac{y^k(y^k)^T}{(s^k)^T y^k}-\frac{B^k s^k(B^ks^k)^T}{(s^k)^T B^ks^k}$
$H^{k&#43;1}=(I-\rho_k y^k(s^k)^T)^TH^{k}(I-\rho_k y^k(s^k)^T)&#43;\rho_ks^k(s^k)^T, \rho=\frac{1}{s^T y}$


DFP方法，和BFGS为对偶关系

$Wy^k=s^k$




收敛性质

Zoutendijk 条件：满足Wolfe准则的一般迭代格式，有下界、连续可微、梯度利普希茨连续，则$\sum_{k=0}^\infty \cos^2(\theta_k)||\nabla f(x^k)||^2&lt;\infty$，$\cos\theta_k=\frac{-\nabla f(x^k)^T d^k}{||\nabla f(x^k)^T ||||d^k||}$
BFGS 全局收敛性：初始矩阵$B^0$对称正定，目标函数连续可微，对$f(x^0)$下水平集凸，且存在正数$m$以及$M$对任意$x,z$有$m||z||^2\le z^T \nabla ^2 f(x)z \le M||z||^2$，则 BFGS 格式结合 Wolfe 线搜索的拟牛顿算法全局收敛到极小值点
BFGS 收敛速度：目标二阶连续可微，最优点邻域Hessian矩阵利普希茨连续，BFGS收敛，误差之和小于正无穷，则Q-超线性收敛





约束优化最优性理论

拉格朗日函数$L(x,\lambda,\nu)=f(x)&#43;\sum_{i\in I} \lambda_i c_i(x)&#43;\sum_{i \in E} \nu_i c_i(x)$
对偶函数$g(\lambda, \nu)=\inf_x L(x,\lambda,\nu)$是凸函数，给出原优化问题的下界$g(\lambda,\nu)\le p^*$
最优下界$\max g(\lambda,\nu)=max_{\lambda\ge 0,v}\inf_x L(x,\lambda,\nu)$

$domg = {(λ,ν) | λ ≥ 0,g(λ,ν) &gt; −∞}$，当 $(λ, ν) ∈ \mathrm{dom}  g$ 时，称为对偶可行解，对偶问题的最优值为 $q^∗$.称 $p^∗ − q^∗(≥ 0)$ 为对偶间隙，对偶间隙为零，则强对偶原理成立


拉格朗日函数不动点$\nabla_x L(x^,\lambda_1^)=0$是必需但不充分的
某点$x^$不存在一阶可行下降方向时，$\nabla_x L(x^,\lambda_1^)=0,\lambda_1^\ge 0$且(互补松弛条件：)$\lambda_1^c_1(x^)=0$
切锥$T_X(x)$：切向量$d=\lim_{k\rightarrow \infty}\frac{z_k-x}{t_k}$的集合，最优化要求切锥(可行方向集合)不包含使得目标函数值下降的方向
几何最优性条件：对局部极小点的可行点，目标和约束函数可微，则$d^T\nabla f(x^)\ge 0, \forall d \in T_X(x^)$&lt;=&gt;$T_X(x^)\cap{d|\nabla f(x^)^T d&lt;0}=\varnothing$
线性化可行锥：$F(x)={d|d^T∇c_i(x) = 0, ∀ i ∈ E； d^T∇c_i(x)≤0,∀i∈A(x)∩I}$，积极集$A(x)=E∪{i∈I : c_i(x)=0}$
线性无关约束规格：给定可行点 $x$ 及相应的积极集 $A(x)$. 如果积极集对应的约束函数的梯度, 即 $∇c_i(x), i ∈ A(x)$, 是线性无关的, 则称线性无关约束规格 (LICQ) 在点 $x$ 处成立，如果LICQ 成立，则有 $T_X (x) = F (x)$
MFCQ：如果存在一个向量 $w ∈ R^n$, 使得$∇c_i(x)^Tw &lt; 0, ∀i ∈ A(x) ∩ I;∇c_i(x)^Tw = 0, ∀i ∈ E$，并且等式约束对应的梯度集 ${∇c_i(x), i ∈ E}$是线性无关的，则称 MFCQ 在点 x 处成立
KKT条件：（如果局部极小点处有$T_X (x^∗) = F (x^∗)$）

稳定性条件$\nabla_x L(x^,\lambda^)=\nabla f(x^)&#43;\sum_{i\in I\cup E} \lambda_i^\nabla c_i(x^*)=0$
原始可行性条件 $c_i (x^∗) = 0, ∀i ∈ E,$^
原始可行性条件 $c_i (x^∗) ⩽ 0, ∀i ∈ I$
对偶可行性条件 $λ^∗_i ⩾0,∀i∈I$
互补松弛条件 $λ^∗_i c_i (x^∗) = 0,∀i ∈ I$


二阶最优性条件：

二阶必要条件：如果局部最优解处处有$T_X (x^∗) = F (x^∗)$，$(x^,\lambda^)$满足KKT条件，则$d^T∇^2_{xx}L(x^∗,λ^∗)d ⩾ 0, ∀d ∈ C (x^∗,λ^∗)$
二阶充分条件：$d^T∇^2_{xx}L(x^∗,λ^∗)d&gt;0, ∀d∈C(x^∗,λ^∗),d\ne0$，那么 $x^∗$ 为一个严格局部极小解.




约束优化方法


二次罚函数法

等式二次罚函数

$P_E(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)，\sigma&gt;0$
给定 σ1 &gt; 0,x0,k ← 1.罚因子增长系数 ρ &gt; 1;
while 未达到收敛准则 do
以 xk 为初始点，求解 x[k&#43;1] = argmin PE (x, σk);
选取 σ[k&#43;1] = ρ*σ[k];
k ← k &#43; 1;
end
收敛性：

设 $x^{k&#43;1}$ 是 $P_E (x, σ^k)$ 的全局极小解, $σ^k$ 单调上升趋于无穷, 则 $x^k$ 的每个极限点$x^∗$都是原问题的全局极小解
$\sigma c_i\rightarrow -\lambda_i^*$（一定条件下）




不等式二次罚函数

$P_I(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in I}\tilde{c}_i^2(x)，\sigma&gt;0, \tilde{c}_i(x)=\max{c_i(x),0}$


一般约束的二次罚函数

$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i^2(x)&#43;\sum{i\in E}c_i^2(x))$





内点罚函数（常用对数）

$P_I(x,\sigma)=f(x)-\sigma \sum_{i\in I}\ln(-c_i(x))$( 罚因子逐渐缩小，系数$\rho$)
收敛性：$|\sigma_k\sum_{i \in I}(-c_i(x^{k&#43;1})|\le \varepsilon$（实际上极限为0）



精确罚函数法

$l_1$罚函数：$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i(x)&#43;\sum{i\in E}|c_i(x)|)$
当罚因子充分大 $σ&gt;||λ^*||_∞$(不需要是正无穷) 时，原问题的极小值点就是罚函数的极小值点

增广拉格朗日函数法

等式约束

增广拉格朗日函数$L_\sigma(x,\lambda)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)$
初始坐标、乘子、罚因子及其更新常数，约束违反常数，精度，迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 等式约束满足精度 then 返回近似解，终止
else 更新乘子、罚因子
end
罚因子更新$σ_{k&#43;1} = ρσ{k}$，乘子更新$λ^{k&#43;1}_i=λ^k_i&#43;σ_i c_i (x^{k&#43;1})$


一般约束约束

引入松弛变量，$L(x,s,\lambda,\mu)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43; \sum_{i\in I}\mu_i (c_i(x)&#43;s_i)$，$s_i\ge 0$；$p(x,s)=\sum_{i\in E}c_i^2(x)&#43;\sum_{i \in I}(c_i(x)&#43;s_i)^2$
增广拉格朗日函数：$L_\sigma (x,s,\lambda,\mu)=L&#43;p(x,s)$
取最优的$s_i=\max{-\frac{\mu_i}{\sigma_k}-c_i(x),0}$，原问题等价于优化$L_\sigma (x,\lambda,\mu)$
初始坐标、乘子、罚因子及其更新常数，约束违反常数e，精度，常数alpha和beta、迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 约束违反度小与ek then
if 约束违反度小于违反度常数e 且梯度范数小于精度 then
返回近似解，终止
else 更新两个乘子、罚因子不变，减小精度条件和约束违反度
else 乘子不变，更新罚因子，调整误差和约束违反度
end
乘子更新$E:\lambda_i^{k&#43;1}=\lambda_i^k \sigma_k c_i(x^{k&#43;1})$，$I:\mu_i^{k&#43;1}=\max{\mu_i^k &#43;\sigma_k c_i(x^{k&#43;1}),0}$
误差和约束违反度：$\eta_{k&#43;1}=\frac{\eta_k}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{\varepsilon_k}{\sigma_{k&#43;1}^\beta}$或$\eta_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}^\alpha}$


凸优化问题



交替方向乘子法ADMM

对于优化$f_1(x)&#43;f_2(x), A_1x_1&#43;A_2x_2=b$，
增广拉格朗日函数$L_\rho(x_1,x_2,y)=f_1(x_1)&#43;f_2(x_2)&#43;y^T(A_1x_1&#43;A_2x_2-b)&#43;\frac{\rho}{2}||A_1x_1&#43;A_2x_2-b||^2_2$，
[[乘子更新]]$y^{k&#43;1}=y^k&#43;\tau\rho (A_1x_1^{k&#43;1}&#43;A_2x_2^{k&#43;1}-b)$
交替求极小：$x_1^{k&#43;1}=\argmin L_\rho (x_1,x_2^k,y^k)$；$x_2^{k&#43;1}=\argmin L_\rho (x_1^{k&#43;1},x_2,y^k)$；[[乘子更新]]





随机一阶优化方法


随机梯度类算法

随机梯度法

迭代格式$x^{k&#43;1}=x^k-\eta_k \nabla f_{ik}(x^k)$


小批量随机梯度法

迭代格式$x^{k&#43;1}=x^k-\eta\nabla f _{S_k}(x^k), \nabla f {S_k}(x^k)=\frac{1}{|S_k|}\sum{i\in S_k }\nabla f_i(x^k)$





随机动量法

迭代格式：$v^k=\beta_k v^{k-1}&#43;\nabla f_{i_k}(x^k), x^{k&#43;1}=x^k-\eta_k v^k$
等价于重球法$x^{k&#43;1}=x^k-\eta_k \nabla f_{i_k}(x^k)&#43;\hat{\beta}_k(x^k-x^{k-1})$



随机次梯度法

迭代格式： $x^{k&#43;1} = x^k − η_kg^k, g^k ∈ ∂f_{i_k} (x^k)$,
当满足$\sum \eta_k =&#43;\infty, \frac{\sum_{1\sim K-1}}{\eta_k^2}{\sum_{1\sim K-1}}{\eta_k}\rightarrow 0$时算法收敛
函数的渐近表现很脆弱，这种算法结构很难实现并行化，当问题规模较大时，算法执行时间长



随机方差缩减类方法

$SGD_$：$x^{k&#43;1}=x^k-\eta(\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^))$

动态抽样方法

范数测试、内积测试、锐角测试
分层抽样方法：将训练样本分类，每一类独立采样子集

$x^{k&#43;1}=x^k-\frac{\eta_k}{n}\sum_{i=1}^{t}\frac{n_i^k}{b_i^k}\sum_{s\in B_i^k} \nabla f_s(x^k)$





SAG算法

全梯度的估计$\bar{g}^k=\frac{1}{n}\sum_{j=1}^n v_j^k$
迭代时$v_j^{k&#43;1}=\nabla f_{i_k}(x_k)\quad \mathrm{if} j=i_k ,\mathrm{else}: v_j^k$，即更新之后将抽取的样本对应的随机梯度改为当前的随机梯度值
由于每次只有一部分改变，可以写成$\bar{g}^k=\bar{g}^{k-1}-\frac{1}{n}v_{i_k}^{k-1}&#43;\frac{1}{n}v_{i_k}^k$



SAGA算法

SAGA 算法选择一个参考点$\bar{x}^i,v_i=\nabla f_i(\bar{x}^i)$
$g^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(\bar{x}^{i_k})&#43;\frac{1}{n}\sum_{j=1}^n \nabla f_j(\bar{x}_j)$
x
线性收敛速度



SVRG算法

每经过几次迭代之后设置检查点，计算全梯度作为参考

$\nabla f(x^j)=\frac{1}{n}\sum_{i=1}^n \nabla f_i(x^j)$
$v^k=\nabla f_{i_k}(x^k)-(\nabla f_{i_k}(x^j)-\nabla f(x^j))$


对于参考点的函数值期望的意义下线性收敛速度



随机递归梯度法SARAH

梯度估计的更新$v^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^{k-1})&#43;v^{k-1} , v^0=$全梯度
不是无偏估计



带BB步长的方差缩减类


AdaGrad

$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{G^k&#43;\varepsilon 1_n}}\circ g^k$
$G^{k&#43;1}=G^k&#43;g^k \circ g^k$



RMSProp

$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{M^k&#43;\varepsilon 1_n}}\circ g^k$
$G^{k&#43;1}=\rho G^k&#43;(1-\rho)g^{k&#43;1} \circ g^{k&#43;1}$



Adam

梯度$g^k=\nabla f_i (x^k)$
一阶矩$S^k=\rho_1 S^{k-1}&#43;(1-\rho_1)g^k$
二阶矩$M^k=\rho_2 M^{k-1}&#43;(1-\rho_2)g^k\circ g^k$
一阶矩修正$\hat{S}^k=\frac{S^k}{1-\rho_1^k}$
二阶矩修正$\hat{M}^k=\frac{M^k}{1-\rho_2^k}$
$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{\hat{M}^k&#43;\varepsilon 1_n}}\circ \hat{S}^k$



AdaBelief

修改二阶矩的计算

$Q^k=\rho_2 Q^{k-1}&#43;(1-\rho_2)(g^k-S^k)\circ (g^k-S^k)$


修正二阶矩偏差时加入额外的$\varepsilon$保证有下界

$\hat{Q}^k=\frac{Q^k&#43;\varepsilon}{1-\rho_2^k}$


AdaBelief 算法在“大梯度，小曲率”情况下有优势




">
<meta name="author" content="">
<link rel="canonical" href="https://sjj1017.github.io/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://sjj1017.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://sjj1017.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://sjj1017.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://sjj1017.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://sjj1017.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://sjj1017.github.io/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

    <script type="text/javascript">
        document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
            delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "\\[", right: "\\]", display: true},
              {left: "$", right: "$", display: false},
              {left: "\\(", right: "\\)", display: false},
            ],
          });
        });
      </script><meta property="og:title" content="人工智能的数学基础" />
<meta property="og:description" content="引言

经验风险$R_{emp}(f(u,x)=\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)$
期望风险(真实风险)：$R_{exp}(f(u, x)) = \mathbb{E}[l(f(u, x), v)]$
结构风险模型：$R_{srm}\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)&#43;\lambda J(f)$
全体数据集最好算法$f^$，有限样本有限算法集最佳算法$\hat{h}_H$，全体数据有限算法最佳$h_H^$
近似误差$R_{exp}(h_H^)-R^$，估算误差$R_{emp}(\hat{h}H) − R{exp}(h^∗_H)$

最优化基础


广义实值函数

基本概念

广义实值函数：映射$\mathbb{R}^n$-&gt;广义函数空间$\mathbb{R}\cup{\pm\infty}$
$\alpha$-下水平集：$C_\alpha={x|f(x)\le\alpha}$，上方图$\mathrm{epi}$ $f = { (x, t) ∈ R^{n&#43;1} |f(x) ≤ t}$
$\alpha$-下水平集是闭集&lt;=&gt;下半连续&lt;=&gt;闭函数（上方图是闭集）
对偶范数$||y||*=sup{||x||\le1}x^Ty$
梯度$\nabla f(x)=[\frac{\partial f}{\partial x_1}(x),&hellip;,\frac{\partial f}{\partial x_n}(x)]^T$，Hessian矩阵（$n\times n$）:$\nabla^2 f(x)$
方向导数$\partial f(x;d)=\frac{\partial d}{\partial d}(x)=\lim_{\theta\rightarrow 0 }\frac{f(x&#43;\theta d)-f(x)}{\theta}=\nabla f(x)^T d$
二阶方向导数$d^T\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\frac{\partial f_i}{\partial x_j}(x)$
泰勒展开式：$f(x&#43;d)=f(x)&#43;\nabla f(x&#43;td)^T d=f(x)&#43;\nabla f(x)^T d&#43;\frac{1}{2}d^T\nabla^2 f(x&#43;td)d$
凸性

凸集：$\eta x_1&#43;(1-\eta) x_2\in S$
凸函数$f(\eta x_1 &#43;(1-\eta)x_2 \le \eta f(x_1)&#43;(1-\eta)f(x_2)$&lt;=&gt;$f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)$&lt;=&gt;当且仅当在任意直线上是凸的
强凸：$\exists \mu&gt;0, f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)&#43;\frac{1}{2}\mu ||x_2-x_1||_2^2$
二阶条件$\nabla ^2 f(x)\ge 0$



利普希茨连续

存在$L$，对于任意的$x,y\in \mathrm{dom} f$有：$||\nabla f(x)-\nabla f(y)|\le L||x-y|||$&lt;=&gt;$||\nabla ^2 f(x)||\le L, \forall x$
凸函数，满足利普希茨条件，则$||\nabla f(x)-\nabla f(y)||^2\le L(x-y)^T(\nabla f(x)-\nabla f(y))$
#三个等价条件


次梯度

$f(y)\ge f(x)&#43;g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\partial f(x)$


共轭函数

$f^*(y)=sup_x{y^Tx-f(x)}$

性质：$f(x)&#43;f*(y)\ge x^Ty$，若$f$为闭函数，$f^{**}=f$









优化算法与基本结构

算法基本结构

全局最小点$f(x^)&lt;f(x)$、严格全局最小点$x^\ne x$
线搜索算法:
给定初始点x0∈R，置k:=0
若在 x[k] 点终止准则成立，则 x[k] 即为求得的最优解，终止; 否则，转步 3
根据方向计算规则，求得 x[k] 点搜索方向 d[k]
根据步长计算规则，求得搜索步长 η[k]
令x[k&#43;1]=x[k]&#43;η[k]*d[k]，置k:=k&#43;1，转步2
终止准则：$||g^k||\le \varepsilon$或$||x^{k&#43;1}-x^k||&lt;\varepsilon$或$||f(x^{k&#43;1})-f(x^k)||&lt;\varepsilon$


收敛速度

若$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||}=\beta$，$0=\beta$超线性收敛，$0&lt;\beta&lt;1$线性收敛，$\beta=1$次线性收敛
二次收敛$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||^2}=\beta$(任意常数)
存在$\alpha\ge 1,\beta &gt;0$，当$k$足够大（与$\alpha \beta$无关），恒有$||x^{k&#43;1}-x^||\le \beta ||x^k-x^||^\alpha$
如果他对于任意正定二次函数，从任意初始点出发，可以经有限步迭代求得极小点，我们就称该算法具有二次终止性





线搜索技术

精确线搜索法

Armojo准则：$d^k$是$x^k处$的下降方向，若$f(x^k&#43;\eta d^k)\le f(x^k)&#43;\rho \eta \nabla f(x^k)^T d^k$，则$\eta$满足Armijo准则
Armijo线搜索算法

选择初始步长 η，参数 ρ,γ ∈ (0,1)，初始化 η ← ηˆ
若 ηk 满足Armijo准则，则终止计算，得步长 ηk. 否则，转步
令ηk :=γηk，转步2.


Goldstein准则：在Armijo准则基础上加上$f(x^k&#43;\eta d^k)\ge f(x^k)&#43;(1-\rho) \eta \nabla f(x^k)^T d^k$


非精确线搜索

Wolfe 准则，它的核心思想有两个：目标函数值应该有足够的下降；可接受点处的切线斜率 ≥ 初始斜率的 σ 倍
在Armijo准则上加伤$\nabla f(x^k&#43;\eta d^k)^T d^k\ge \sigma \nabla f(x^k)^T d^k$
非精确线搜索步长的存在性：$f(x^k &#43; ηd^k)$ 在 $η &gt; 0$ 时有下界，且 $∇f(x^k)^Td^k &lt; 0$





最优化分支

线性与非线性规划

线性规划LP：在线性等式和不等式约束下最优化一个线性目标函数
如果约束和目标函数中有一个非线性的，则问题就称为非线性规划问题


二次规划QP

目标函数是变量的二次函数
Q半正定时QP是凸优化问题，可以用内点法在多项式时间内求解


锥优化CO

非负性条件 $x ≥ 0$ 用锥包含约束替换后得到的优化问题
二阶锥$x_1^2 ⩾ x_2^2 &#43;···&#43;x^2_n,x_1 ⩾ 0$
对称半正定锥 $X=X^T$半正定


整数规划ILP

部分或全部变量取整数的优化问题
0-1规划
混合整数规划：既有连续变量又有整数约束变量时，问题称为混合整数线性规划


动态规划

涉及递推关系的计算方法，把问题分成阶段以便进行递推优化






最优化理论

Weierstrass 定理：条件任意成立一个：$\mathrm{dom f}$有界；存在常数$\bar{gamma}$使得下水平集$C_\gamma$是非空且有界的；$f$是强制的，即对于任意满足极限为$&#43;\infty$的点列都有其函数值趋向于$&#43;\infty$，则最优化问题的最小点集是非空且紧的

无约束可微优化问题

下降方向：如果存在$d$满足$\nabla f(x)^Td&lt;0$则$d$为一个下降方向。局部最优点处不能有下降方向。局部极小点$x^$满足$\nabla f(x^)=0$(一阶必要条件)，同时$\nabla^2f(x^*)$半正定（二阶必要条件），如果二阶连续可微，那么二阶必要条件是充分条件。
假设$f$#适当 且凸，则$x^$是局部极小点&lt;=&gt;$0\in \partial f(x^)$
对于二阶连续可微的目标函数，梯度法、牛顿法、拟牛顿法在每一次迭代均能看做是构建局部的二次模型，梯度法可以看做利用 $(1/η^k)I$作为Hessian矩阵估计，牛顿类算法利用真实Hessian矩阵，拟牛顿利用真实Hessian矩阵或逆的估计构建模型。牛顿法收敛最快计算量存储量大，梯度法相对最慢。

梯度类算法

一般形式：$x^{k&#43;1}=x^k&#43;\eta_k d^k$，收敛速度：$L$-利普希茨连续时$0&lt;\eta&lt;\frac{1}{L}$时为$O(1/k)$，对强凸函数$0&lt;\eta&lt;\frac{1}{L&#43;\eta}$时Q-线性收敛
精确线搜索、数值线性搜索法
BB方法：

选取$min||\eta y^{k-1}-s^{k-1}||^2$或$min|| y^{k-1}-\eta^{-1}s^{k-1}||^2$的解
$s^{k-1}=x^{k&#43;1}-x^k$，$y^{k-1}=\nabla f(x^{k&#43;1})-\nabla f(x^k)$
解分别为$\eta_{BB1}^k=\frac{(s^{k-1})^Ty^{k-1}} {(y^{k-1})^Ty^{k-1}}$，$\eta_{BB2}^k=\frac{(s^{k-1})^Ts^{k-1}} {(s^{k-1})^Ty^{k-1}}$
通过$η_m ⩽η_k ⩽η_M$截断过大或过小的步长，也可以使用两种步长的凸组合





次梯度法

迭代格式：$x^{k&#43;1} = x^k − η^kg^k, g^k ∈ ∂f(x^k)$
若 $0 \notin ∂f(x)$，那么对于任意 $x^∗ ∈ argmin_x f(x)$和任意 $g ∈ ∂f(x)$，存在步长 $η &gt; 0$ 使得$||x−ηg−x^||_2^2 &lt;||x−x^||_2^2$
若至少存在一个极小点且次梯度有界，则$\sum \eta_k(f(x^k)-f(x^))\le \frac{1}{2}||x^0-x^||^2&#43;\frac{1}{2}\sum \eta_k^2 M^2$



经典牛顿法

迭代格式：$x^{k&#43;1} = x^k − \nabla^2f(x^k)^{-1}\nabla f(x^k), g^k ∈ ∂f(x^k)$
极小点处梯度为0，Hessian矩阵正定，则起始点足够近时，收敛是Q-二次的且梯度的范数Q-二次收敛到0



修正牛顿法

迭代格式：$x^{k&#43;1} = x^k &#43;\eta_k d^k$
确定矩阵$E^k$使得$\nabla ^2 f(x^k)&#43;E^k$正定且条件数较小，求解$B^kd^k=-\nabla f(x^k)$，确定步长迭代。



非精确牛顿法

引入残差$r^k=\nabla^2 f(x^k)d^k&#43;\nabla f(x^k)$，$||r^k||\le \alpha_k||\nabla f(x^k)||$
若存在$t&lt;1$使得$0&lt;\alpha_k&lt;t$则Q-线性收敛；若$\alpha_k$收敛到0，则Q-超线性收敛；若$\alpha_k=O(||\nabla f(x^k)||)$，则Q-二次收敛



拟牛顿条件

Hessian的近似矩阵满足$y^k=B^{k&#43;1}s^k$，逆矩阵$s^k=H^{k&#43;1}y^k$
迭代格式：$x^{k&#43;1}=x^k&#43;\alpha_k d^k$，$d^k=-(B^k)^{-1}\nabla f(x^k)=-H^k\nabla f(x^k)$
SR1秩一更新

$B^{k&#43;1}=B^k&#43;\frac{(y^k-B^ks^k)(y^k-B^ks^k)^T}{(y^k-B^ks^k)^T s^k}$
$H^{k&#43;1}=H^k&#43;\frac{(s^k-H^ky^k)(s^k-H^ky^k)^T}{(s^k-H^ky^k)^T y^k}$


秩二更新

BFGS(相当于在满足割线方程的对称矩阵中找到离 $H^k$ 最近的矩阵)

利用割线方程$Ws^k=y^k$
$B^{k&#43;1}=B^k&#43;\frac{y^k(y^k)^T}{(s^k)^T y^k}-\frac{B^k s^k(B^ks^k)^T}{(s^k)^T B^ks^k}$
$H^{k&#43;1}=(I-\rho_k y^k(s^k)^T)^TH^{k}(I-\rho_k y^k(s^k)^T)&#43;\rho_ks^k(s^k)^T, \rho=\frac{1}{s^T y}$


DFP方法，和BFGS为对偶关系

$Wy^k=s^k$




收敛性质

Zoutendijk 条件：满足Wolfe准则的一般迭代格式，有下界、连续可微、梯度利普希茨连续，则$\sum_{k=0}^\infty \cos^2(\theta_k)||\nabla f(x^k)||^2&lt;\infty$，$\cos\theta_k=\frac{-\nabla f(x^k)^T d^k}{||\nabla f(x^k)^T ||||d^k||}$
BFGS 全局收敛性：初始矩阵$B^0$对称正定，目标函数连续可微，对$f(x^0)$下水平集凸，且存在正数$m$以及$M$对任意$x,z$有$m||z||^2\le z^T \nabla ^2 f(x)z \le M||z||^2$，则 BFGS 格式结合 Wolfe 线搜索的拟牛顿算法全局收敛到极小值点
BFGS 收敛速度：目标二阶连续可微，最优点邻域Hessian矩阵利普希茨连续，BFGS收敛，误差之和小于正无穷，则Q-超线性收敛





约束优化最优性理论

拉格朗日函数$L(x,\lambda,\nu)=f(x)&#43;\sum_{i\in I} \lambda_i c_i(x)&#43;\sum_{i \in E} \nu_i c_i(x)$
对偶函数$g(\lambda, \nu)=\inf_x L(x,\lambda,\nu)$是凸函数，给出原优化问题的下界$g(\lambda,\nu)\le p^*$
最优下界$\max g(\lambda,\nu)=max_{\lambda\ge 0,v}\inf_x L(x,\lambda,\nu)$

$domg = {(λ,ν) | λ ≥ 0,g(λ,ν) &gt; −∞}$，当 $(λ, ν) ∈ \mathrm{dom}  g$ 时，称为对偶可行解，对偶问题的最优值为 $q^∗$.称 $p^∗ − q^∗(≥ 0)$ 为对偶间隙，对偶间隙为零，则强对偶原理成立


拉格朗日函数不动点$\nabla_x L(x^,\lambda_1^)=0$是必需但不充分的
某点$x^$不存在一阶可行下降方向时，$\nabla_x L(x^,\lambda_1^)=0,\lambda_1^\ge 0$且(互补松弛条件：)$\lambda_1^c_1(x^)=0$
切锥$T_X(x)$：切向量$d=\lim_{k\rightarrow \infty}\frac{z_k-x}{t_k}$的集合，最优化要求切锥(可行方向集合)不包含使得目标函数值下降的方向
几何最优性条件：对局部极小点的可行点，目标和约束函数可微，则$d^T\nabla f(x^)\ge 0, \forall d \in T_X(x^)$&lt;=&gt;$T_X(x^)\cap{d|\nabla f(x^)^T d&lt;0}=\varnothing$
线性化可行锥：$F(x)={d|d^T∇c_i(x) = 0, ∀ i ∈ E； d^T∇c_i(x)≤0,∀i∈A(x)∩I}$，积极集$A(x)=E∪{i∈I : c_i(x)=0}$
线性无关约束规格：给定可行点 $x$ 及相应的积极集 $A(x)$. 如果积极集对应的约束函数的梯度, 即 $∇c_i(x), i ∈ A(x)$, 是线性无关的, 则称线性无关约束规格 (LICQ) 在点 $x$ 处成立，如果LICQ 成立，则有 $T_X (x) = F (x)$
MFCQ：如果存在一个向量 $w ∈ R^n$, 使得$∇c_i(x)^Tw &lt; 0, ∀i ∈ A(x) ∩ I;∇c_i(x)^Tw = 0, ∀i ∈ E$，并且等式约束对应的梯度集 ${∇c_i(x), i ∈ E}$是线性无关的，则称 MFCQ 在点 x 处成立
KKT条件：（如果局部极小点处有$T_X (x^∗) = F (x^∗)$）

稳定性条件$\nabla_x L(x^,\lambda^)=\nabla f(x^)&#43;\sum_{i\in I\cup E} \lambda_i^\nabla c_i(x^*)=0$
原始可行性条件 $c_i (x^∗) = 0, ∀i ∈ E,$^
原始可行性条件 $c_i (x^∗) ⩽ 0, ∀i ∈ I$
对偶可行性条件 $λ^∗_i ⩾0,∀i∈I$
互补松弛条件 $λ^∗_i c_i (x^∗) = 0,∀i ∈ I$


二阶最优性条件：

二阶必要条件：如果局部最优解处处有$T_X (x^∗) = F (x^∗)$，$(x^,\lambda^)$满足KKT条件，则$d^T∇^2_{xx}L(x^∗,λ^∗)d ⩾ 0, ∀d ∈ C (x^∗,λ^∗)$
二阶充分条件：$d^T∇^2_{xx}L(x^∗,λ^∗)d&gt;0, ∀d∈C(x^∗,λ^∗),d\ne0$，那么 $x^∗$ 为一个严格局部极小解.




约束优化方法


二次罚函数法

等式二次罚函数

$P_E(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)，\sigma&gt;0$
给定 σ1 &gt; 0,x0,k ← 1.罚因子增长系数 ρ &gt; 1;
while 未达到收敛准则 do
以 xk 为初始点，求解 x[k&#43;1] = argmin PE (x, σk);
选取 σ[k&#43;1] = ρ*σ[k];
k ← k &#43; 1;
end
收敛性：

设 $x^{k&#43;1}$ 是 $P_E (x, σ^k)$ 的全局极小解, $σ^k$ 单调上升趋于无穷, 则 $x^k$ 的每个极限点$x^∗$都是原问题的全局极小解
$\sigma c_i\rightarrow -\lambda_i^*$（一定条件下）




不等式二次罚函数

$P_I(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in I}\tilde{c}_i^2(x)，\sigma&gt;0, \tilde{c}_i(x)=\max{c_i(x),0}$


一般约束的二次罚函数

$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i^2(x)&#43;\sum{i\in E}c_i^2(x))$





内点罚函数（常用对数）

$P_I(x,\sigma)=f(x)-\sigma \sum_{i\in I}\ln(-c_i(x))$( 罚因子逐渐缩小，系数$\rho$)
收敛性：$|\sigma_k\sum_{i \in I}(-c_i(x^{k&#43;1})|\le \varepsilon$（实际上极限为0）



精确罚函数法

$l_1$罚函数：$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i(x)&#43;\sum{i\in E}|c_i(x)|)$
当罚因子充分大 $σ&gt;||λ^*||_∞$(不需要是正无穷) 时，原问题的极小值点就是罚函数的极小值点

增广拉格朗日函数法

等式约束

增广拉格朗日函数$L_\sigma(x,\lambda)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)$
初始坐标、乘子、罚因子及其更新常数，约束违反常数，精度，迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 等式约束满足精度 then 返回近似解，终止
else 更新乘子、罚因子
end
罚因子更新$σ_{k&#43;1} = ρσ{k}$，乘子更新$λ^{k&#43;1}_i=λ^k_i&#43;σ_i c_i (x^{k&#43;1})$


一般约束约束

引入松弛变量，$L(x,s,\lambda,\mu)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43; \sum_{i\in I}\mu_i (c_i(x)&#43;s_i)$，$s_i\ge 0$；$p(x,s)=\sum_{i\in E}c_i^2(x)&#43;\sum_{i \in I}(c_i(x)&#43;s_i)^2$
增广拉格朗日函数：$L_\sigma (x,s,\lambda,\mu)=L&#43;p(x,s)$
取最优的$s_i=\max{-\frac{\mu_i}{\sigma_k}-c_i(x),0}$，原问题等价于优化$L_\sigma (x,\lambda,\mu)$
初始坐标、乘子、罚因子及其更新常数，约束违反常数e，精度，常数alpha和beta、迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 约束违反度小与ek then
if 约束违反度小于违反度常数e 且梯度范数小于精度 then
返回近似解，终止
else 更新两个乘子、罚因子不变，减小精度条件和约束违反度
else 乘子不变，更新罚因子，调整误差和约束违反度
end
乘子更新$E:\lambda_i^{k&#43;1}=\lambda_i^k \sigma_k c_i(x^{k&#43;1})$，$I:\mu_i^{k&#43;1}=\max{\mu_i^k &#43;\sigma_k c_i(x^{k&#43;1}),0}$
误差和约束违反度：$\eta_{k&#43;1}=\frac{\eta_k}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{\varepsilon_k}{\sigma_{k&#43;1}^\beta}$或$\eta_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}^\alpha}$


凸优化问题



交替方向乘子法ADMM

对于优化$f_1(x)&#43;f_2(x), A_1x_1&#43;A_2x_2=b$，
增广拉格朗日函数$L_\rho(x_1,x_2,y)=f_1(x_1)&#43;f_2(x_2)&#43;y^T(A_1x_1&#43;A_2x_2-b)&#43;\frac{\rho}{2}||A_1x_1&#43;A_2x_2-b||^2_2$，
[[乘子更新]]$y^{k&#43;1}=y^k&#43;\tau\rho (A_1x_1^{k&#43;1}&#43;A_2x_2^{k&#43;1}-b)$
交替求极小：$x_1^{k&#43;1}=\argmin L_\rho (x_1,x_2^k,y^k)$；$x_2^{k&#43;1}=\argmin L_\rho (x_1^{k&#43;1},x_2,y^k)$；[[乘子更新]]





随机一阶优化方法


随机梯度类算法

随机梯度法

迭代格式$x^{k&#43;1}=x^k-\eta_k \nabla f_{ik}(x^k)$


小批量随机梯度法

迭代格式$x^{k&#43;1}=x^k-\eta\nabla f _{S_k}(x^k), \nabla f {S_k}(x^k)=\frac{1}{|S_k|}\sum{i\in S_k }\nabla f_i(x^k)$





随机动量法

迭代格式：$v^k=\beta_k v^{k-1}&#43;\nabla f_{i_k}(x^k), x^{k&#43;1}=x^k-\eta_k v^k$
等价于重球法$x^{k&#43;1}=x^k-\eta_k \nabla f_{i_k}(x^k)&#43;\hat{\beta}_k(x^k-x^{k-1})$



随机次梯度法

迭代格式： $x^{k&#43;1} = x^k − η_kg^k, g^k ∈ ∂f_{i_k} (x^k)$,
当满足$\sum \eta_k =&#43;\infty, \frac{\sum_{1\sim K-1}}{\eta_k^2}{\sum_{1\sim K-1}}{\eta_k}\rightarrow 0$时算法收敛
函数的渐近表现很脆弱，这种算法结构很难实现并行化，当问题规模较大时，算法执行时间长



随机方差缩减类方法

$SGD_$：$x^{k&#43;1}=x^k-\eta(\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^))$

动态抽样方法

范数测试、内积测试、锐角测试
分层抽样方法：将训练样本分类，每一类独立采样子集

$x^{k&#43;1}=x^k-\frac{\eta_k}{n}\sum_{i=1}^{t}\frac{n_i^k}{b_i^k}\sum_{s\in B_i^k} \nabla f_s(x^k)$





SAG算法

全梯度的估计$\bar{g}^k=\frac{1}{n}\sum_{j=1}^n v_j^k$
迭代时$v_j^{k&#43;1}=\nabla f_{i_k}(x_k)\quad \mathrm{if} j=i_k ,\mathrm{else}: v_j^k$，即更新之后将抽取的样本对应的随机梯度改为当前的随机梯度值
由于每次只有一部分改变，可以写成$\bar{g}^k=\bar{g}^{k-1}-\frac{1}{n}v_{i_k}^{k-1}&#43;\frac{1}{n}v_{i_k}^k$



SAGA算法

SAGA 算法选择一个参考点$\bar{x}^i,v_i=\nabla f_i(\bar{x}^i)$
$g^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(\bar{x}^{i_k})&#43;\frac{1}{n}\sum_{j=1}^n \nabla f_j(\bar{x}_j)$
x
线性收敛速度



SVRG算法

每经过几次迭代之后设置检查点，计算全梯度作为参考

$\nabla f(x^j)=\frac{1}{n}\sum_{i=1}^n \nabla f_i(x^j)$
$v^k=\nabla f_{i_k}(x^k)-(\nabla f_{i_k}(x^j)-\nabla f(x^j))$


对于参考点的函数值期望的意义下线性收敛速度



随机递归梯度法SARAH

梯度估计的更新$v^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^{k-1})&#43;v^{k-1} , v^0=$全梯度
不是无偏估计



带BB步长的方差缩减类


AdaGrad

$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{G^k&#43;\varepsilon 1_n}}\circ g^k$
$G^{k&#43;1}=G^k&#43;g^k \circ g^k$



RMSProp

$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{M^k&#43;\varepsilon 1_n}}\circ g^k$
$G^{k&#43;1}=\rho G^k&#43;(1-\rho)g^{k&#43;1} \circ g^{k&#43;1}$



Adam

梯度$g^k=\nabla f_i (x^k)$
一阶矩$S^k=\rho_1 S^{k-1}&#43;(1-\rho_1)g^k$
二阶矩$M^k=\rho_2 M^{k-1}&#43;(1-\rho_2)g^k\circ g^k$
一阶矩修正$\hat{S}^k=\frac{S^k}{1-\rho_1^k}$
二阶矩修正$\hat{M}^k=\frac{M^k}{1-\rho_2^k}$
$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{\hat{M}^k&#43;\varepsilon 1_n}}\circ \hat{S}^k$



AdaBelief

修改二阶矩的计算

$Q^k=\rho_2 Q^{k-1}&#43;(1-\rho_2)(g^k-S^k)\circ (g^k-S^k)$


修正二阶矩偏差时加入额外的$\varepsilon$保证有下界

$\hat{Q}^k=\frac{Q^k&#43;\varepsilon}{1-\rho_2^k}$


AdaBelief 算法在“大梯度，小曲率”情况下有优势




" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sjj1017.github.io/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-19T16:34:43+08:00" />
<meta property="article:modified_time" content="2024-10-19T16:34:43+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="人工智能的数学基础"/>
<meta name="twitter:description" content="引言

经验风险$R_{emp}(f(u,x)=\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)$
期望风险(真实风险)：$R_{exp}(f(u, x)) = \mathbb{E}[l(f(u, x), v)]$
结构风险模型：$R_{srm}\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)&#43;\lambda J(f)$
全体数据集最好算法$f^$，有限样本有限算法集最佳算法$\hat{h}_H$，全体数据有限算法最佳$h_H^$
近似误差$R_{exp}(h_H^)-R^$，估算误差$R_{emp}(\hat{h}H) − R{exp}(h^∗_H)$

最优化基础


广义实值函数

基本概念

广义实值函数：映射$\mathbb{R}^n$-&gt;广义函数空间$\mathbb{R}\cup{\pm\infty}$
$\alpha$-下水平集：$C_\alpha={x|f(x)\le\alpha}$，上方图$\mathrm{epi}$ $f = { (x, t) ∈ R^{n&#43;1} |f(x) ≤ t}$
$\alpha$-下水平集是闭集&lt;=&gt;下半连续&lt;=&gt;闭函数（上方图是闭集）
对偶范数$||y||*=sup{||x||\le1}x^Ty$
梯度$\nabla f(x)=[\frac{\partial f}{\partial x_1}(x),&hellip;,\frac{\partial f}{\partial x_n}(x)]^T$，Hessian矩阵（$n\times n$）:$\nabla^2 f(x)$
方向导数$\partial f(x;d)=\frac{\partial d}{\partial d}(x)=\lim_{\theta\rightarrow 0 }\frac{f(x&#43;\theta d)-f(x)}{\theta}=\nabla f(x)^T d$
二阶方向导数$d^T\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\frac{\partial f_i}{\partial x_j}(x)$
泰勒展开式：$f(x&#43;d)=f(x)&#43;\nabla f(x&#43;td)^T d=f(x)&#43;\nabla f(x)^T d&#43;\frac{1}{2}d^T\nabla^2 f(x&#43;td)d$
凸性

凸集：$\eta x_1&#43;(1-\eta) x_2\in S$
凸函数$f(\eta x_1 &#43;(1-\eta)x_2 \le \eta f(x_1)&#43;(1-\eta)f(x_2)$&lt;=&gt;$f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)$&lt;=&gt;当且仅当在任意直线上是凸的
强凸：$\exists \mu&gt;0, f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)&#43;\frac{1}{2}\mu ||x_2-x_1||_2^2$
二阶条件$\nabla ^2 f(x)\ge 0$



利普希茨连续

存在$L$，对于任意的$x,y\in \mathrm{dom} f$有：$||\nabla f(x)-\nabla f(y)|\le L||x-y|||$&lt;=&gt;$||\nabla ^2 f(x)||\le L, \forall x$
凸函数，满足利普希茨条件，则$||\nabla f(x)-\nabla f(y)||^2\le L(x-y)^T(\nabla f(x)-\nabla f(y))$
#三个等价条件


次梯度

$f(y)\ge f(x)&#43;g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\partial f(x)$


共轭函数

$f^*(y)=sup_x{y^Tx-f(x)}$

性质：$f(x)&#43;f*(y)\ge x^Ty$，若$f$为闭函数，$f^{**}=f$









优化算法与基本结构

算法基本结构

全局最小点$f(x^)&lt;f(x)$、严格全局最小点$x^\ne x$
线搜索算法:
给定初始点x0∈R，置k:=0
若在 x[k] 点终止准则成立，则 x[k] 即为求得的最优解，终止; 否则，转步 3
根据方向计算规则，求得 x[k] 点搜索方向 d[k]
根据步长计算规则，求得搜索步长 η[k]
令x[k&#43;1]=x[k]&#43;η[k]*d[k]，置k:=k&#43;1，转步2
终止准则：$||g^k||\le \varepsilon$或$||x^{k&#43;1}-x^k||&lt;\varepsilon$或$||f(x^{k&#43;1})-f(x^k)||&lt;\varepsilon$


收敛速度

若$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||}=\beta$，$0=\beta$超线性收敛，$0&lt;\beta&lt;1$线性收敛，$\beta=1$次线性收敛
二次收敛$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||^2}=\beta$(任意常数)
存在$\alpha\ge 1,\beta &gt;0$，当$k$足够大（与$\alpha \beta$无关），恒有$||x^{k&#43;1}-x^||\le \beta ||x^k-x^||^\alpha$
如果他对于任意正定二次函数，从任意初始点出发，可以经有限步迭代求得极小点，我们就称该算法具有二次终止性





线搜索技术

精确线搜索法

Armojo准则：$d^k$是$x^k处$的下降方向，若$f(x^k&#43;\eta d^k)\le f(x^k)&#43;\rho \eta \nabla f(x^k)^T d^k$，则$\eta$满足Armijo准则
Armijo线搜索算法

选择初始步长 η，参数 ρ,γ ∈ (0,1)，初始化 η ← ηˆ
若 ηk 满足Armijo准则，则终止计算，得步长 ηk. 否则，转步
令ηk :=γηk，转步2.


Goldstein准则：在Armijo准则基础上加上$f(x^k&#43;\eta d^k)\ge f(x^k)&#43;(1-\rho) \eta \nabla f(x^k)^T d^k$


非精确线搜索

Wolfe 准则，它的核心思想有两个：目标函数值应该有足够的下降；可接受点处的切线斜率 ≥ 初始斜率的 σ 倍
在Armijo准则上加伤$\nabla f(x^k&#43;\eta d^k)^T d^k\ge \sigma \nabla f(x^k)^T d^k$
非精确线搜索步长的存在性：$f(x^k &#43; ηd^k)$ 在 $η &gt; 0$ 时有下界，且 $∇f(x^k)^Td^k &lt; 0$





最优化分支

线性与非线性规划

线性规划LP：在线性等式和不等式约束下最优化一个线性目标函数
如果约束和目标函数中有一个非线性的，则问题就称为非线性规划问题


二次规划QP

目标函数是变量的二次函数
Q半正定时QP是凸优化问题，可以用内点法在多项式时间内求解


锥优化CO

非负性条件 $x ≥ 0$ 用锥包含约束替换后得到的优化问题
二阶锥$x_1^2 ⩾ x_2^2 &#43;···&#43;x^2_n,x_1 ⩾ 0$
对称半正定锥 $X=X^T$半正定


整数规划ILP

部分或全部变量取整数的优化问题
0-1规划
混合整数规划：既有连续变量又有整数约束变量时，问题称为混合整数线性规划


动态规划

涉及递推关系的计算方法，把问题分成阶段以便进行递推优化






最优化理论

Weierstrass 定理：条件任意成立一个：$\mathrm{dom f}$有界；存在常数$\bar{gamma}$使得下水平集$C_\gamma$是非空且有界的；$f$是强制的，即对于任意满足极限为$&#43;\infty$的点列都有其函数值趋向于$&#43;\infty$，则最优化问题的最小点集是非空且紧的

无约束可微优化问题

下降方向：如果存在$d$满足$\nabla f(x)^Td&lt;0$则$d$为一个下降方向。局部最优点处不能有下降方向。局部极小点$x^$满足$\nabla f(x^)=0$(一阶必要条件)，同时$\nabla^2f(x^*)$半正定（二阶必要条件），如果二阶连续可微，那么二阶必要条件是充分条件。
假设$f$#适当 且凸，则$x^$是局部极小点&lt;=&gt;$0\in \partial f(x^)$
对于二阶连续可微的目标函数，梯度法、牛顿法、拟牛顿法在每一次迭代均能看做是构建局部的二次模型，梯度法可以看做利用 $(1/η^k)I$作为Hessian矩阵估计，牛顿类算法利用真实Hessian矩阵，拟牛顿利用真实Hessian矩阵或逆的估计构建模型。牛顿法收敛最快计算量存储量大，梯度法相对最慢。

梯度类算法

一般形式：$x^{k&#43;1}=x^k&#43;\eta_k d^k$，收敛速度：$L$-利普希茨连续时$0&lt;\eta&lt;\frac{1}{L}$时为$O(1/k)$，对强凸函数$0&lt;\eta&lt;\frac{1}{L&#43;\eta}$时Q-线性收敛
精确线搜索、数值线性搜索法
BB方法：

选取$min||\eta y^{k-1}-s^{k-1}||^2$或$min|| y^{k-1}-\eta^{-1}s^{k-1}||^2$的解
$s^{k-1}=x^{k&#43;1}-x^k$，$y^{k-1}=\nabla f(x^{k&#43;1})-\nabla f(x^k)$
解分别为$\eta_{BB1}^k=\frac{(s^{k-1})^Ty^{k-1}} {(y^{k-1})^Ty^{k-1}}$，$\eta_{BB2}^k=\frac{(s^{k-1})^Ts^{k-1}} {(s^{k-1})^Ty^{k-1}}$
通过$η_m ⩽η_k ⩽η_M$截断过大或过小的步长，也可以使用两种步长的凸组合





次梯度法

迭代格式：$x^{k&#43;1} = x^k − η^kg^k, g^k ∈ ∂f(x^k)$
若 $0 \notin ∂f(x)$，那么对于任意 $x^∗ ∈ argmin_x f(x)$和任意 $g ∈ ∂f(x)$，存在步长 $η &gt; 0$ 使得$||x−ηg−x^||_2^2 &lt;||x−x^||_2^2$
若至少存在一个极小点且次梯度有界，则$\sum \eta_k(f(x^k)-f(x^))\le \frac{1}{2}||x^0-x^||^2&#43;\frac{1}{2}\sum \eta_k^2 M^2$



经典牛顿法

迭代格式：$x^{k&#43;1} = x^k − \nabla^2f(x^k)^{-1}\nabla f(x^k), g^k ∈ ∂f(x^k)$
极小点处梯度为0，Hessian矩阵正定，则起始点足够近时，收敛是Q-二次的且梯度的范数Q-二次收敛到0



修正牛顿法

迭代格式：$x^{k&#43;1} = x^k &#43;\eta_k d^k$
确定矩阵$E^k$使得$\nabla ^2 f(x^k)&#43;E^k$正定且条件数较小，求解$B^kd^k=-\nabla f(x^k)$，确定步长迭代。



非精确牛顿法

引入残差$r^k=\nabla^2 f(x^k)d^k&#43;\nabla f(x^k)$，$||r^k||\le \alpha_k||\nabla f(x^k)||$
若存在$t&lt;1$使得$0&lt;\alpha_k&lt;t$则Q-线性收敛；若$\alpha_k$收敛到0，则Q-超线性收敛；若$\alpha_k=O(||\nabla f(x^k)||)$，则Q-二次收敛



拟牛顿条件

Hessian的近似矩阵满足$y^k=B^{k&#43;1}s^k$，逆矩阵$s^k=H^{k&#43;1}y^k$
迭代格式：$x^{k&#43;1}=x^k&#43;\alpha_k d^k$，$d^k=-(B^k)^{-1}\nabla f(x^k)=-H^k\nabla f(x^k)$
SR1秩一更新

$B^{k&#43;1}=B^k&#43;\frac{(y^k-B^ks^k)(y^k-B^ks^k)^T}{(y^k-B^ks^k)^T s^k}$
$H^{k&#43;1}=H^k&#43;\frac{(s^k-H^ky^k)(s^k-H^ky^k)^T}{(s^k-H^ky^k)^T y^k}$


秩二更新

BFGS(相当于在满足割线方程的对称矩阵中找到离 $H^k$ 最近的矩阵)

利用割线方程$Ws^k=y^k$
$B^{k&#43;1}=B^k&#43;\frac{y^k(y^k)^T}{(s^k)^T y^k}-\frac{B^k s^k(B^ks^k)^T}{(s^k)^T B^ks^k}$
$H^{k&#43;1}=(I-\rho_k y^k(s^k)^T)^TH^{k}(I-\rho_k y^k(s^k)^T)&#43;\rho_ks^k(s^k)^T, \rho=\frac{1}{s^T y}$


DFP方法，和BFGS为对偶关系

$Wy^k=s^k$




收敛性质

Zoutendijk 条件：满足Wolfe准则的一般迭代格式，有下界、连续可微、梯度利普希茨连续，则$\sum_{k=0}^\infty \cos^2(\theta_k)||\nabla f(x^k)||^2&lt;\infty$，$\cos\theta_k=\frac{-\nabla f(x^k)^T d^k}{||\nabla f(x^k)^T ||||d^k||}$
BFGS 全局收敛性：初始矩阵$B^0$对称正定，目标函数连续可微，对$f(x^0)$下水平集凸，且存在正数$m$以及$M$对任意$x,z$有$m||z||^2\le z^T \nabla ^2 f(x)z \le M||z||^2$，则 BFGS 格式结合 Wolfe 线搜索的拟牛顿算法全局收敛到极小值点
BFGS 收敛速度：目标二阶连续可微，最优点邻域Hessian矩阵利普希茨连续，BFGS收敛，误差之和小于正无穷，则Q-超线性收敛





约束优化最优性理论

拉格朗日函数$L(x,\lambda,\nu)=f(x)&#43;\sum_{i\in I} \lambda_i c_i(x)&#43;\sum_{i \in E} \nu_i c_i(x)$
对偶函数$g(\lambda, \nu)=\inf_x L(x,\lambda,\nu)$是凸函数，给出原优化问题的下界$g(\lambda,\nu)\le p^*$
最优下界$\max g(\lambda,\nu)=max_{\lambda\ge 0,v}\inf_x L(x,\lambda,\nu)$

$domg = {(λ,ν) | λ ≥ 0,g(λ,ν) &gt; −∞}$，当 $(λ, ν) ∈ \mathrm{dom}  g$ 时，称为对偶可行解，对偶问题的最优值为 $q^∗$.称 $p^∗ − q^∗(≥ 0)$ 为对偶间隙，对偶间隙为零，则强对偶原理成立


拉格朗日函数不动点$\nabla_x L(x^,\lambda_1^)=0$是必需但不充分的
某点$x^$不存在一阶可行下降方向时，$\nabla_x L(x^,\lambda_1^)=0,\lambda_1^\ge 0$且(互补松弛条件：)$\lambda_1^c_1(x^)=0$
切锥$T_X(x)$：切向量$d=\lim_{k\rightarrow \infty}\frac{z_k-x}{t_k}$的集合，最优化要求切锥(可行方向集合)不包含使得目标函数值下降的方向
几何最优性条件：对局部极小点的可行点，目标和约束函数可微，则$d^T\nabla f(x^)\ge 0, \forall d \in T_X(x^)$&lt;=&gt;$T_X(x^)\cap{d|\nabla f(x^)^T d&lt;0}=\varnothing$
线性化可行锥：$F(x)={d|d^T∇c_i(x) = 0, ∀ i ∈ E； d^T∇c_i(x)≤0,∀i∈A(x)∩I}$，积极集$A(x)=E∪{i∈I : c_i(x)=0}$
线性无关约束规格：给定可行点 $x$ 及相应的积极集 $A(x)$. 如果积极集对应的约束函数的梯度, 即 $∇c_i(x), i ∈ A(x)$, 是线性无关的, 则称线性无关约束规格 (LICQ) 在点 $x$ 处成立，如果LICQ 成立，则有 $T_X (x) = F (x)$
MFCQ：如果存在一个向量 $w ∈ R^n$, 使得$∇c_i(x)^Tw &lt; 0, ∀i ∈ A(x) ∩ I;∇c_i(x)^Tw = 0, ∀i ∈ E$，并且等式约束对应的梯度集 ${∇c_i(x), i ∈ E}$是线性无关的，则称 MFCQ 在点 x 处成立
KKT条件：（如果局部极小点处有$T_X (x^∗) = F (x^∗)$）

稳定性条件$\nabla_x L(x^,\lambda^)=\nabla f(x^)&#43;\sum_{i\in I\cup E} \lambda_i^\nabla c_i(x^*)=0$
原始可行性条件 $c_i (x^∗) = 0, ∀i ∈ E,$^
原始可行性条件 $c_i (x^∗) ⩽ 0, ∀i ∈ I$
对偶可行性条件 $λ^∗_i ⩾0,∀i∈I$
互补松弛条件 $λ^∗_i c_i (x^∗) = 0,∀i ∈ I$


二阶最优性条件：

二阶必要条件：如果局部最优解处处有$T_X (x^∗) = F (x^∗)$，$(x^,\lambda^)$满足KKT条件，则$d^T∇^2_{xx}L(x^∗,λ^∗)d ⩾ 0, ∀d ∈ C (x^∗,λ^∗)$
二阶充分条件：$d^T∇^2_{xx}L(x^∗,λ^∗)d&gt;0, ∀d∈C(x^∗,λ^∗),d\ne0$，那么 $x^∗$ 为一个严格局部极小解.




约束优化方法


二次罚函数法

等式二次罚函数

$P_E(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)，\sigma&gt;0$
给定 σ1 &gt; 0,x0,k ← 1.罚因子增长系数 ρ &gt; 1;
while 未达到收敛准则 do
以 xk 为初始点，求解 x[k&#43;1] = argmin PE (x, σk);
选取 σ[k&#43;1] = ρ*σ[k];
k ← k &#43; 1;
end
收敛性：

设 $x^{k&#43;1}$ 是 $P_E (x, σ^k)$ 的全局极小解, $σ^k$ 单调上升趋于无穷, 则 $x^k$ 的每个极限点$x^∗$都是原问题的全局极小解
$\sigma c_i\rightarrow -\lambda_i^*$（一定条件下）




不等式二次罚函数

$P_I(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in I}\tilde{c}_i^2(x)，\sigma&gt;0, \tilde{c}_i(x)=\max{c_i(x),0}$


一般约束的二次罚函数

$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i^2(x)&#43;\sum{i\in E}c_i^2(x))$





内点罚函数（常用对数）

$P_I(x,\sigma)=f(x)-\sigma \sum_{i\in I}\ln(-c_i(x))$( 罚因子逐渐缩小，系数$\rho$)
收敛性：$|\sigma_k\sum_{i \in I}(-c_i(x^{k&#43;1})|\le \varepsilon$（实际上极限为0）



精确罚函数法

$l_1$罚函数：$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i(x)&#43;\sum{i\in E}|c_i(x)|)$
当罚因子充分大 $σ&gt;||λ^*||_∞$(不需要是正无穷) 时，原问题的极小值点就是罚函数的极小值点

增广拉格朗日函数法

等式约束

增广拉格朗日函数$L_\sigma(x,\lambda)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)$
初始坐标、乘子、罚因子及其更新常数，约束违反常数，精度，迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 等式约束满足精度 then 返回近似解，终止
else 更新乘子、罚因子
end
罚因子更新$σ_{k&#43;1} = ρσ{k}$，乘子更新$λ^{k&#43;1}_i=λ^k_i&#43;σ_i c_i (x^{k&#43;1})$


一般约束约束

引入松弛变量，$L(x,s,\lambda,\mu)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43; \sum_{i\in I}\mu_i (c_i(x)&#43;s_i)$，$s_i\ge 0$；$p(x,s)=\sum_{i\in E}c_i^2(x)&#43;\sum_{i \in I}(c_i(x)&#43;s_i)^2$
增广拉格朗日函数：$L_\sigma (x,s,\lambda,\mu)=L&#43;p(x,s)$
取最优的$s_i=\max{-\frac{\mu_i}{\sigma_k}-c_i(x),0}$，原问题等价于优化$L_\sigma (x,\lambda,\mu)$
初始坐标、乘子、罚因子及其更新常数，约束违反常数e，精度，常数alpha和beta、迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 约束违反度小与ek then
if 约束违反度小于违反度常数e 且梯度范数小于精度 then
返回近似解，终止
else 更新两个乘子、罚因子不变，减小精度条件和约束违反度
else 乘子不变，更新罚因子，调整误差和约束违反度
end
乘子更新$E:\lambda_i^{k&#43;1}=\lambda_i^k \sigma_k c_i(x^{k&#43;1})$，$I:\mu_i^{k&#43;1}=\max{\mu_i^k &#43;\sigma_k c_i(x^{k&#43;1}),0}$
误差和约束违反度：$\eta_{k&#43;1}=\frac{\eta_k}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{\varepsilon_k}{\sigma_{k&#43;1}^\beta}$或$\eta_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}^\alpha}$


凸优化问题



交替方向乘子法ADMM

对于优化$f_1(x)&#43;f_2(x), A_1x_1&#43;A_2x_2=b$，
增广拉格朗日函数$L_\rho(x_1,x_2,y)=f_1(x_1)&#43;f_2(x_2)&#43;y^T(A_1x_1&#43;A_2x_2-b)&#43;\frac{\rho}{2}||A_1x_1&#43;A_2x_2-b||^2_2$，
[[乘子更新]]$y^{k&#43;1}=y^k&#43;\tau\rho (A_1x_1^{k&#43;1}&#43;A_2x_2^{k&#43;1}-b)$
交替求极小：$x_1^{k&#43;1}=\argmin L_\rho (x_1,x_2^k,y^k)$；$x_2^{k&#43;1}=\argmin L_\rho (x_1^{k&#43;1},x_2,y^k)$；[[乘子更新]]





随机一阶优化方法


随机梯度类算法

随机梯度法

迭代格式$x^{k&#43;1}=x^k-\eta_k \nabla f_{ik}(x^k)$


小批量随机梯度法

迭代格式$x^{k&#43;1}=x^k-\eta\nabla f _{S_k}(x^k), \nabla f {S_k}(x^k)=\frac{1}{|S_k|}\sum{i\in S_k }\nabla f_i(x^k)$





随机动量法

迭代格式：$v^k=\beta_k v^{k-1}&#43;\nabla f_{i_k}(x^k), x^{k&#43;1}=x^k-\eta_k v^k$
等价于重球法$x^{k&#43;1}=x^k-\eta_k \nabla f_{i_k}(x^k)&#43;\hat{\beta}_k(x^k-x^{k-1})$



随机次梯度法

迭代格式： $x^{k&#43;1} = x^k − η_kg^k, g^k ∈ ∂f_{i_k} (x^k)$,
当满足$\sum \eta_k =&#43;\infty, \frac{\sum_{1\sim K-1}}{\eta_k^2}{\sum_{1\sim K-1}}{\eta_k}\rightarrow 0$时算法收敛
函数的渐近表现很脆弱，这种算法结构很难实现并行化，当问题规模较大时，算法执行时间长



随机方差缩减类方法

$SGD_$：$x^{k&#43;1}=x^k-\eta(\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^))$

动态抽样方法

范数测试、内积测试、锐角测试
分层抽样方法：将训练样本分类，每一类独立采样子集

$x^{k&#43;1}=x^k-\frac{\eta_k}{n}\sum_{i=1}^{t}\frac{n_i^k}{b_i^k}\sum_{s\in B_i^k} \nabla f_s(x^k)$





SAG算法

全梯度的估计$\bar{g}^k=\frac{1}{n}\sum_{j=1}^n v_j^k$
迭代时$v_j^{k&#43;1}=\nabla f_{i_k}(x_k)\quad \mathrm{if} j=i_k ,\mathrm{else}: v_j^k$，即更新之后将抽取的样本对应的随机梯度改为当前的随机梯度值
由于每次只有一部分改变，可以写成$\bar{g}^k=\bar{g}^{k-1}-\frac{1}{n}v_{i_k}^{k-1}&#43;\frac{1}{n}v_{i_k}^k$



SAGA算法

SAGA 算法选择一个参考点$\bar{x}^i,v_i=\nabla f_i(\bar{x}^i)$
$g^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(\bar{x}^{i_k})&#43;\frac{1}{n}\sum_{j=1}^n \nabla f_j(\bar{x}_j)$
x
线性收敛速度



SVRG算法

每经过几次迭代之后设置检查点，计算全梯度作为参考

$\nabla f(x^j)=\frac{1}{n}\sum_{i=1}^n \nabla f_i(x^j)$
$v^k=\nabla f_{i_k}(x^k)-(\nabla f_{i_k}(x^j)-\nabla f(x^j))$


对于参考点的函数值期望的意义下线性收敛速度



随机递归梯度法SARAH

梯度估计的更新$v^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^{k-1})&#43;v^{k-1} , v^0=$全梯度
不是无偏估计



带BB步长的方差缩减类


AdaGrad

$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{G^k&#43;\varepsilon 1_n}}\circ g^k$
$G^{k&#43;1}=G^k&#43;g^k \circ g^k$



RMSProp

$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{M^k&#43;\varepsilon 1_n}}\circ g^k$
$G^{k&#43;1}=\rho G^k&#43;(1-\rho)g^{k&#43;1} \circ g^{k&#43;1}$



Adam

梯度$g^k=\nabla f_i (x^k)$
一阶矩$S^k=\rho_1 S^{k-1}&#43;(1-\rho_1)g^k$
二阶矩$M^k=\rho_2 M^{k-1}&#43;(1-\rho_2)g^k\circ g^k$
一阶矩修正$\hat{S}^k=\frac{S^k}{1-\rho_1^k}$
二阶矩修正$\hat{M}^k=\frac{M^k}{1-\rho_2^k}$
$x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{\hat{M}^k&#43;\varepsilon 1_n}}\circ \hat{S}^k$



AdaBelief

修改二阶矩的计算

$Q^k=\rho_2 Q^{k-1}&#43;(1-\rho_2)(g^k-S^k)\circ (g^k-S^k)$


修正二阶矩偏差时加入额外的$\varepsilon$保证有下界

$\hat{Q}^k=\frac{Q^k&#43;\varepsilon}{1-\rho_2^k}$


AdaBelief 算法在“大梯度，小曲率”情况下有优势




"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sjj1017.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "人工智能的数学基础",
      "item": "https://sjj1017.github.io/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "人工智能的数学基础",
  "name": "人工智能的数学基础",
  "description": "引言 经验风险$R_{emp}(f(u,x)=\\frac{1}{n}\\sum_{i=1}^n l(f(u_i,x),v_i)$ 期望风险(真实风险)：$R_{exp}(f(u, x)) = \\mathbb{E}[l(f(u, x), v)]$ 结构风险模型：$R_{srm}\\frac{1}{n}\\sum_{i=1}^n l(f(u_i,x),v_i)+\\lambda J(f)$ 全体数据集最好算法$f^$，有限样本有限算法集最佳算法$\\hat{h}_H$，全体数据有限算法最佳$h_H^$ 近似误差$R_{exp}(h_H^)-R^$，估算误差$R_{emp}(\\hat{h}H) − R{exp}(h^∗_H)$ 最优化基础 广义实值函数 基本概念 广义实值函数：映射$\\mathbb{R}^n$-\u0026gt;广义函数空间$\\mathbb{R}\\cup{\\pm\\infty}$ $\\alpha$-下水平集：$C_\\alpha={x|f(x)\\le\\alpha}$，上方图$\\mathrm{epi}$ $f = { (x, t) ∈ R^{n+1} |f(x) ≤ t}$ $\\alpha$-下水平集是闭集\u0026lt;=\u0026gt;下半连续\u0026lt;=\u0026gt;闭函数（上方图是闭集） 对偶范数$||y||*=sup{||x||\\le1}x^Ty$ 梯度$\\nabla f(x)=[\\frac{\\partial f}{\\partial x_1}(x),\u0026hellip;,\\frac{\\partial f}{\\partial x_n}(x)]^T$，Hessian矩阵（$n\\times n$）:$\\nabla^2 f(x)$ 方向导数$\\partial f(x;d)=\\frac{\\partial d}{\\partial d}(x)=\\lim_{\\theta\\rightarrow 0 }\\frac{f(x+\\theta d)-f(x)}{\\theta}=\\nabla f(x)^T d$ 二阶方向导数$d^T\\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\\frac{\\partial f_i}{\\partial x_j}(x)$ 泰勒展开式：$f(x+d)=f(x)+\\nabla f(x+td)^T d=f(x)+\\nabla f(x)^T d+\\frac{1}{2}d^T\\nabla^2 f(x+td)d$ 凸性 凸集：$\\eta x_1+(1-\\eta) x_2\\in S$ 凸函数$f(\\eta x_1 +(1-\\eta)x_2 \\le \\eta f(x_1)+(1-\\eta)f(x_2)$\u0026lt;=\u0026gt;$f(y)\\ge f(x)+\\nabla f(x)^T(y-x)$\u0026lt;=\u0026gt;当且仅当在任意直线上是凸的 强凸：$\\exists \\mu\u0026gt;0, f(y)\\ge f(x)+\\nabla f(x)^T(y-x)+\\frac{1}{2}\\mu ||x_2-x_1||_2^2$ 二阶条件$\\nabla ^2 f(x)\\ge 0$ 利普希茨连续 存在$L$，对于任意的$x,y\\in \\mathrm{dom} f$有：$||\\nabla f(x)-\\nabla f(y)|\\le L||x-y|||$\u0026lt;=\u0026gt;$||\\nabla ^2 f(x)||\\le L, \\forall x$ 凸函数，满足利普希茨条件，则$||\\nabla f(x)-\\nabla f(y)||^2\\le L(x-y)^T(\\nabla f(x)-\\nabla f(y))$ #三个等价条件 次梯度 $f(y)\\ge f(x)+g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\\partial f(x)$ 共轭函数 $f^*(y)=sup_x{y^Tx-f(x)}$ 性质：$f(x)+f*(y)\\ge x^Ty$，若$f$为闭函数，$f^{**}=f$ 优化算法与基本结构 算法基本结构 全局最小点$f(x^)\u0026lt;f(x)$、严格全局最小点$x^\\ne x$ 线搜索算法: 给定初始点x0∈R，置k:=0 若在 x[k] 点终止准则成立，则 x[k] 即为求得的最优解，终止; 否则，转步 3 根据方向计算规则，求得 x[k] 点搜索方向 d[k] 根据步长计算规则，求得搜索步长 η[k] 令x[k+1]=x[k]+η[k]*d[k]，置k:=k+1，转步2 终止准则：$||g^k||\\le \\varepsilon$或$||x^{k+1}-x^k||\u0026lt;\\varepsilon$或$||f(x^{k+1})-f(x^k)||\u0026lt;\\varepsilon$ 收敛速度 若$\\lim \\frac{||x^{k+1}-x^||}{||x^k-x^||}=\\beta$，$0=\\beta$超线性收敛，$0\u0026lt;\\beta\u0026lt;1$线性收敛，$\\beta=1$次线性收敛 二次收敛$\\lim \\frac{||x^{k+1}-x^||}{||x^k-x^||^2}=\\beta$(任意常数) 存在$\\alpha\\ge 1,\\beta \u0026gt;0$，当$k$足够大（与$\\alpha \\beta$无关），恒有$||x^{k+1}-x^||\\le \\beta ||x^k-x^||^\\alpha$ 如果他对于任意正定二次函数，从任意初始点出发，可以经有限步迭代求得极小点，我们就称该算法具有二次终止性 线搜索技术 精确线搜索法 Armojo准则：$d^k$是$x^k处$的下降方向，若$f(x^k+\\eta d^k)\\le f(x^k)+\\rho \\eta \\nabla f(x^k)^T d^k$，则$\\eta$满足Armijo准则 Armijo线搜索算法 选择初始步长 η，参数 ρ,γ ∈ (0,1)，初始化 η ← ηˆ 若 ηk 满足Armijo准则，则终止计算，得步长 ηk. 否则，转步 令ηk :=γηk，转步2. Goldstein准则：在Armijo准则基础上加上$f(x^k+\\eta d^k)\\ge f(x^k)+(1-\\rho) \\eta \\nabla f(x^k)^T d^k$ 非精确线搜索 Wolfe 准则，它的核心思想有两个：目标函数值应该有足够的下降；可接受点处的切线斜率 ≥ 初始斜率的 σ 倍 在Armijo准则上加伤$\\nabla f(x^k+\\eta d^k)^T d^k\\ge \\sigma \\nabla f(x^k)^T d^k$ 非精确线搜索步长的存在性：$f(x^k + ηd^k)$ 在 $η \u0026gt; 0$ 时有下界，且 $∇f(x^k)^Td^k \u0026lt; 0$ 最优化分支 线性与非线性规划 线性规划LP：在线性等式和不等式约束下最优化一个线性目标函数 如果约束和目标函数中有一个非线性的，则问题就称为非线性规划问题 二次规划QP 目标函数是变量的二次函数 Q半正定时QP是凸优化问题，可以用内点法在多项式时间内求解 锥优化CO 非负性条件 $x ≥ 0$ 用锥包含约束替换后得到的优化问题 二阶锥$x_1^2 ⩾ x_2^2 +···+x^2_n,x_1 ⩾ 0$ 对称半正定锥 $X=X^T$半正定 整数规划ILP 部分或全部变量取整数的优化问题 0-1规划 混合整数规划：既有连续变量又有整数约束变量时，问题称为混合整数线性规划 动态规划 涉及递推关系的计算方法，把问题分成阶段以便进行递推优化 最优化理论 Weierstrass 定理：条件任意成立一个：$\\mathrm{dom f}$有界；存在常数$\\bar{gamma}$使得下水平集$C_\\gamma$是非空且有界的；$f$是强制的，即对于任意满足极限为$+\\infty$的点列都有其函数值趋向于$+\\infty$，则最优化问题的最小点集是非空且紧的 无约束可微优化问题 下降方向：如果存在$d$满足$\\nabla f(x)^Td\u0026lt;0$则$d$为一个下降方向。局部最优点处不能有下降方向。局部极小点$x^$满足$\\nabla f(x^)=0$(一阶必要条件)，同时$\\nabla^2f(x^*)$半正定（二阶必要条件），如果二阶连续可微，那么二阶必要条件是充分条件。 假设$f$#适当 且凸，则$x^$是局部极小点\u0026lt;=\u0026gt;$0\\in \\partial f(x^)$ 对于二阶连续可微的目标函数，梯度法、牛顿法、拟牛顿法在每一次迭代均能看做是构建局部的二次模型，梯度法可以看做利用 $(1/η^k)I$作为Hessian矩阵估计，牛顿类算法利用真实Hessian矩阵，拟牛顿利用真实Hessian矩阵或逆的估计构建模型。牛顿法收敛最快计算量存储量大，梯度法相对最慢。 梯度类算法 一般形式：$x^{k+1}=x^k+\\eta_k d^k$，收敛速度：$L$-利普希茨连续时$0\u0026lt;\\eta\u0026lt;\\frac{1}{L}$时为$O(1/k)$，对强凸函数$0\u0026lt;\\eta\u0026lt;\\frac{1}{L+\\eta}$时Q-线性收敛 精确线搜索、数值线性搜索法 BB方法： 选取$min||\\eta y^{k-1}-s^{k-1}||^2$或$min|| y^{k-1}-\\eta^{-1}s^{k-1}||^2$的解 $s^{k-1}=x^{k+1}-x^k$，$y^{k-1}=\\nabla f(x^{k+1})-\\nabla f(x^k)$ 解分别为$\\eta_{BB1}^k=\\frac{(s^{k-1})^Ty^{k-1}} {(y^{k-1})^Ty^{k-1}}$，$\\eta_{BB2}^k=\\frac{(s^{k-1})^Ts^{k-1}} {(s^{k-1})^Ty^{k-1}}$ 通过$η_m ⩽η_k ⩽η_M$截断过大或过小的步长，也可以使用两种步长的凸组合 次梯度法 迭代格式：$x^{k+1} = x^k − η^kg^k, g^k ∈ ∂f(x^k)$ 若 $0 \\notin ∂f(x)$，那么对于任意 $x^∗ ∈ argmin_x f(x)$和任意 $g ∈ ∂f(x)$，存在步长 $η \u0026gt; 0$ 使得$||x−ηg−x^||_2^2 \u0026lt;||x−x^||_2^2$ 若至少存在一个极小点且次梯度有界，则$\\sum \\eta_k(f(x^k)-f(x^))\\le \\frac{1}{2}||x^0-x^||^2+\\frac{1}{2}\\sum \\eta_k^2 M^2$ 经典牛顿法 迭代格式：$x^{k+1} = x^k − \\nabla^2f(x^k)^{-1}\\nabla f(x^k), g^k ∈ ∂f(x^k)$ 极小点处梯度为0，Hessian矩阵正定，则起始点足够近时，收敛是Q-二次的且梯度的范数Q-二次收敛到0 修正牛顿法 迭代格式：$x^{k+1} = x^k +\\eta_k d^k$ 确定矩阵$E^k$使得$\\nabla ^2 f(x^k)+E^k$正定且条件数较小，求解$B^kd^k=-\\nabla f(x^k)$，确定步长迭代。 非精确牛顿法 引入残差$r^k=\\nabla^2 f(x^k)d^k+\\nabla f(x^k)$，$||r^k||\\le \\alpha_k||\\nabla f(x^k)||$ 若存在$t\u0026lt;1$使得$0\u0026lt;\\alpha_k\u0026lt;t$则Q-线性收敛；若$\\alpha_k$收敛到0，则Q-超线性收敛；若$\\alpha_k=O(||\\nabla f(x^k)||)$，则Q-二次收敛 拟牛顿条件 Hessian的近似矩阵满足$y^k=B^{k+1}s^k$，逆矩阵$s^k=H^{k+1}y^k$ 迭代格式：$x^{k+1}=x^k+\\alpha_k d^k$，$d^k=-(B^k)^{-1}\\nabla f(x^k)=-H^k\\nabla f(x^k)$ SR1秩一更新 $B^{k+1}=B^k+\\frac{(y^k-B^ks^k)(y^k-B^ks^k)^T}{(y^k-B^ks^k)^T s^k}$ $H^{k+1}=H^k+\\frac{(s^k-H^ky^k)(s^k-H^ky^k)^T}{(s^k-H^ky^k)^T y^k}$ 秩二更新 BFGS(相当于在满足割线方程的对称矩阵中找到离 $H^k$ 最近的矩阵) 利用割线方程$Ws^k=y^k$ $B^{k+1}=B^k+\\frac{y^k(y^k)^T}{(s^k)^T y^k}-\\frac{B^k s^k(B^ks^k)^T}{(s^k)^T B^ks^k}$ $H^{k+1}=(I-\\rho_k y^k(s^k)^T)^TH^{k}(I-\\rho_k y^k(s^k)^T)+\\rho_ks^k(s^k)^T, \\rho=\\frac{1}{s^T y}$ DFP方法，和BFGS为对偶关系 $Wy^k=s^k$ 收敛性质 Zoutendijk 条件：满足Wolfe准则的一般迭代格式，有下界、连续可微、梯度利普希茨连续，则$\\sum_{k=0}^\\infty \\cos^2(\\theta_k)||\\nabla f(x^k)||^2\u0026lt;\\infty$，$\\cos\\theta_k=\\frac{-\\nabla f(x^k)^T d^k}{||\\nabla f(x^k)^T ||||d^k||}$ BFGS 全局收敛性：初始矩阵$B^0$对称正定，目标函数连续可微，对$f(x^0)$下水平集凸，且存在正数$m$以及$M$对任意$x,z$有$m||z||^2\\le z^T \\nabla ^2 f(x)z \\le M||z||^2$，则 BFGS 格式结合 Wolfe 线搜索的拟牛顿算法全局收敛到极小值点 BFGS 收敛速度：目标二阶连续可微，最优点邻域Hessian矩阵利普希茨连续，BFGS收敛，误差之和小于正无穷，则Q-超线性收敛 约束优化最优性理论 拉格朗日函数$L(x,\\lambda,\\nu)=f(x)+\\sum_{i\\in I} \\lambda_i c_i(x)+\\sum_{i \\in E} \\nu_i c_i(x)$ 对偶函数$g(\\lambda, \\nu)=\\inf_x L(x,\\lambda,\\nu)$是凸函数，给出原优化问题的下界$g(\\lambda,\\nu)\\le p^*$ 最优下界$\\max g(\\lambda,\\nu)=max_{\\lambda\\ge 0,v}\\inf_x L(x,\\lambda,\\nu)$ $domg = {(λ,ν) | λ ≥ 0,g(λ,ν) \u0026gt; −∞}$，当 $(λ, ν) ∈ \\mathrm{dom} g$ 时，称为对偶可行解，对偶问题的最优值为 $q^∗$.称 $p^∗ − q^∗(≥ 0)$ 为对偶间隙，对偶间隙为零，则强对偶原理成立 拉格朗日函数不动点$\\nabla_x L(x^,\\lambda_1^)=0$是必需但不充分的 某点$x^$不存在一阶可行下降方向时，$\\nabla_x L(x^,\\lambda_1^)=0,\\lambda_1^\\ge 0$且(互补松弛条件：)$\\lambda_1^c_1(x^)=0$ 切锥$T_X(x)$：切向量$d=\\lim_{k\\rightarrow \\infty}\\frac{z_k-x}{t_k}$的集合，最优化要求切锥(可行方向集合)不包含使得目标函数值下降的方向 几何最优性条件：对局部极小点的可行点，目标和约束函数可微，则$d^T\\nabla f(x^)\\ge 0, \\forall d \\in T_X(x^)$\u0026lt;=\u0026gt;$T_X(x^)\\cap{d|\\nabla f(x^)^T d\u0026lt;0}=\\varnothing$ 线性化可行锥：$F(x)={d|d^T∇c_i(x) = 0, ∀ i ∈ E； d^T∇c_i(x)≤0,∀i∈A(x)∩I}$，积极集$A(x)=E∪{i∈I : c_i(x)=0}$ 线性无关约束规格：给定可行点 $x$ 及相应的积极集 $A(x)$. 如果积极集对应的约束函数的梯度, 即 $∇c_i(x), i ∈ A(x)$, 是线性无关的, 则称线性无关约束规格 (LICQ) 在点 $x$ 处成立，如果LICQ 成立，则有 $T_X (x) = F (x)$ MFCQ：如果存在一个向量 $w ∈ R^n$, 使得$∇c_i(x)^Tw \u0026lt; 0, ∀i ∈ A(x) ∩ I;∇c_i(x)^Tw = 0, ∀i ∈ E$，并且等式约束对应的梯度集 ${∇c_i(x), i ∈ E}$是线性无关的，则称 MFCQ 在点 x 处成立 KKT条件：（如果局部极小点处有$T_X (x^∗) = F (x^∗)$） 稳定性条件$\\nabla_x L(x^,\\lambda^)=\\nabla f(x^)+\\sum_{i\\in I\\cup E} \\lambda_i^\\nabla c_i(x^*)=0$ 原始可行性条件 $c_i (x^∗) = 0, ∀i ∈ E,$^ 原始可行性条件 $c_i (x^∗) ⩽ 0, ∀i ∈ I$ 对偶可行性条件 $λ^∗_i ⩾0,∀i∈I$ 互补松弛条件 $λ^∗_i c_i (x^∗) = 0,∀i ∈ I$ 二阶最优性条件： 二阶必要条件：如果局部最优解处处有$T_X (x^∗) = F (x^∗)$，$(x^,\\lambda^)$满足KKT条件，则$d^T∇^2_{xx}L(x^∗,λ^∗)d ⩾ 0, ∀d ∈ C (x^∗,λ^∗)$ 二阶充分条件：$d^T∇^2_{xx}L(x^∗,λ^∗)d\u0026gt;0, ∀d∈C(x^∗,λ^∗),d\\ne0$，那么 $x^∗$ 为一个严格局部极小解. 约束优化方法 二次罚函数法 等式二次罚函数 $P_E(x,\\sigma)=f(x)+\\frac{1}{2}\\sigma \\sum_{i\\in E}c_i^2(x)，\\sigma\u0026gt;0$ 给定 σ1 \u0026gt; 0,x0,k ← 1.罚因子增长系数 ρ \u0026gt; 1; while 未达到收敛准则 do 以 xk 为初始点，求解 x[k+1] = argmin PE (x, σk); 选取 σ[k+1] = ρ*σ[k]; k ← k + 1; end 收敛性： 设 $x^{k+1}$ 是 $P_E (x, σ^k)$ 的全局极小解, $σ^k$ 单调上升趋于无穷, 则 \b$x^k$ 的每个极限点$x^∗$都是原问题的全局极小解 $\\sigma c_i\\rightarrow -\\lambda_i^*$（一定条件下） 不等式二次罚函数 $P_I(x,\\sigma)=f(x)+\\frac{1}{2}\\sigma \\sum_{i\\in I}\\tilde{c}_i^2(x)，\\sigma\u0026gt;0, \\tilde{c}_i(x)=\\max{c_i(x),0}$ 一般约束的二次罚函数 $P(x,\\sigma)=f(x)+\\frac{1}{2}\\sigma (\\sum_{i\\in I}\\tilde{c}i^2(x)+\\sum{i\\in E}c_i^2(x))$ 内点罚函数（常用对数） $P_I(x,\\sigma)=f(x)-\\sigma \\sum_{i\\in I}\\ln(-c_i(x))$( 罚因子逐渐缩小，系数$\\rho$) 收敛性：$|\\sigma_k\\sum_{i \\in I}(-c_i(x^{k+1})|\\le \\varepsilon$（实际上极限为0） 精确罚函数法 $l_1$罚函数：$P(x,\\sigma)=f(x)+\\frac{1}{2}\\sigma (\\sum_{i\\in I}\\tilde{c}i(x)+\\sum{i\\in E}|c_i(x)|)$ 当罚因子充分大 $σ\u0026gt;||λ^*||_∞$(不需要是正无穷) 时，原问题的极小值点就是罚函数的极小值点 增广拉格朗日函数法 等式约束 增广拉格朗日函数$L_\\sigma(x,\\lambda)=f(x)+\\sum_{i\\in E}\\lambda_i c_i(x)+\\frac{1}{2}\\sigma \\sum_{i\\in E}c_i^2(x)$ 初始坐标、乘子、罚因子及其更新常数，约束违反常数，精度，迭代步数 for k=.. do 从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度 if 等式约束满足精度 then 返回近似解，终止 else 更新乘子、罚因子 end 罚因子更新$σ_{k+1} = ρσ{k}$，乘子更新$λ^{k+1}_i=λ^k_i+σ_i c_i (x^{k+1})$ 一般约束约束 引入松弛变量，$L(x,s,\\lambda,\\mu)=f(x)+\\sum_{i\\in E}\\lambda_i c_i(x)+ \\sum_{i\\in I}\\mu_i (c_i(x)+s_i)$，$s_i\\ge 0$；$p(x,s)=\\sum_{i\\in E}c_i^2(x)+\\sum_{i \\in I}(c_i(x)+s_i)^2$ 增广拉格朗日函数：$L_\\sigma (x,s,\\lambda,\\mu)=L+p(x,s)$ 取最优的$s_i=\\max{-\\frac{\\mu_i}{\\sigma_k}-c_i(x),0}$，原问题等价于优化$L_\\sigma (x,\\lambda,\\mu)$ 初始坐标、乘子、罚因子及其更新常数，约束违反常数e，精度，常数alpha和beta、迭代步数 for k=.. do 从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度 if 约束违反度小与ek then if 约束违反度小于违反度常数e 且梯度范数小于精度 then 返回近似解，终止 else 更新两个乘子、罚因子不变，减小精度条件和约束违反度 else 乘子不变，更新罚因子，调整误差和约束违反度 end 乘子更新$E:\\lambda_i^{k+1}=\\lambda_i^k \\sigma_k c_i(x^{k+1})$，$I:\\mu_i^{k+1}=\\max{\\mu_i^k +\\sigma_k c_i(x^{k+1}),0}$ 误差和约束违反度：$\\eta_{k+1}=\\frac{\\eta_k}{\\sigma_{k+1}}，\\varepsilon_{k+1}=\\frac{\\varepsilon_k}{\\sigma_{k+1}^\\beta}$或$\\eta_{k+1}=\\frac{1}{\\sigma_{k+1}}，\\varepsilon_{k+1}=\\frac{1}{\\sigma_{k+1}^\\alpha}$ 凸优化问题 交替方向乘子法ADMM 对于优化$f_1(x)+f_2(x), A_1x_1+A_2x_2=b$， 增广拉格朗日函数$L_\\rho(x_1,x_2,y)=f_1(x_1)+f_2(x_2)+y^T(A_1x_1+A_2x_2-b)+\\frac{\\rho}{2}||A_1x_1+A_2x_2-b||^2_2$， [[乘子更新]]$y^{k+1}=y^k+\\tau\\rho (A_1x_1^{k+1}+A_2x_2^{k+1}-b)$ 交替求极小：$x_1^{k+1}=\\argmin L_\\rho (x_1,x_2^k,y^k)$；$x_2^{k+1}=\\argmin L_\\rho (x_1^{k+1},x_2,y^k)$；[[乘子更新]] 随机一阶优化方法 随机梯度类算法 随机梯度法 迭代格式$x^{k+1}=x^k-\\eta_k \\nabla f_{ik}(x^k)$ 小批量随机梯度法 迭代格式$x^{k+1}=x^k-\\eta\\nabla f _{S_k}(x^k), \\nabla f {S_k}(x^k)=\\frac{1}{|S_k|}\\sum{i\\in S_k }\\nabla f_i(x^k)$ 随机动量法 迭代格式：$v^k=\\beta_k v^{k-1}+\\nabla f_{i_k}(x^k), x^{k+1}=x^k-\\eta_k v^k$ 等价于重球法$x^{k+1}=x^k-\\eta_k \\nabla f_{i_k}(x^k)+\\hat{\\beta}_k(x^k-x^{k-1})$ 随机次梯度法 迭代格式： $x^{k+1} = x^k − η_kg^k, g^k ∈ ∂f_{i_k} (x^k)$, 当满足$\\sum \\eta_k =+\\infty, \\frac{\\sum_{1\\sim K-1}}{\\eta_k^2}{\\sum_{1\\sim K-1}}{\\eta_k}\\rightarrow 0$时算法收敛 函数的渐近表现很脆弱，这种算法结构很难实现并行化，当问题规模较大时，算法执行时间长 随机方差缩减类方法 $SGD_$：$x^{k+1}=x^k-\\eta(\\nabla f_{i_k}(x^k)-\\nabla f_{i_k}(x^))$ 动态抽样方法 范数测试、内积测试、锐角测试 分层抽样方法：将训练样本分类，每一类独立采样子集 $x^{k+1}=x^k-\\frac{\\eta_k}{n}\\sum_{i=1}^{t}\\frac{n_i^k}{b_i^k}\\sum_{s\\in B_i^k} \\nabla f_s(x^k)$ SAG算法 全梯度的估计$\\bar{g}^k=\\frac{1}{n}\\sum_{j=1}^n v_j^k$ 迭代时$v_j^{k+1}=\\nabla f_{i_k}(x_k)\\quad \\mathrm{if} j=i_k ,\\mathrm{else}: v_j^k$，即更新之后将抽取的样本对应的随机梯度改为当前的随机梯度值 由于每次只有一部分改变，可以写成$\\bar{g}^k=\\bar{g}^{k-1}-\\frac{1}{n}v_{i_k}^{k-1}+\\frac{1}{n}v_{i_k}^k$ SAGA算法 SAGA 算法选择一个参考点$\\bar{x}^i,v_i=\\nabla f_i(\\bar{x}^i)$ $g^k=\\nabla f_{i_k}(x^k)-\\nabla f_{i_k}(\\bar{x}^{i_k})+\\frac{1}{n}\\sum_{j=1}^n \\nabla f_j(\\bar{x}_j)$ x 线性收敛速度 SVRG算法 每经过几次迭代之后设置检查点，计算全梯度作为参考 $\\nabla f(x^j)=\\frac{1}{n}\\sum_{i=1}^n \\nabla f_i(x^j)$ $v^k=\\nabla f_{i_k}(x^k)-(\\nabla f_{i_k}(x^j)-\\nabla f(x^j))$ 对于参考点的函数值期望的意义下线性收敛速度 随机递归梯度法SARAH 梯度估计的更新$v^k=\\nabla f_{i_k}(x^k)-\\nabla f_{i_k}(x^{k-1})+v^{k-1} , v^0=$全梯度 不是无偏估计 带BB步长的方差缩减类 AdaGrad $x^{k+1}=x^k-\\frac{\\eta}{\\sqrt{G^k+\\varepsilon 1_n}}\\circ g^k$ $G^{k+1}=G^k+g^k \\circ g^k$ RMSProp $x^{k+1}=x^k-\\frac{\\eta}{\\sqrt{M^k+\\varepsilon 1_n}}\\circ g^k$ $G^{k+1}=\\rho G^k+(1-\\rho)g^{k+1} \\circ g^{k+1}$ Adam 梯度$g^k=\\nabla f_i (x^k)$ 一阶矩$S^k=\\rho_1 S^{k-1}+(1-\\rho_1)g^k$ 二阶矩$M^k=\\rho_2 M^{k-1}+(1-\\rho_2)g^k\\circ g^k$ 一阶矩修正$\\hat{S}^k=\\frac{S^k}{1-\\rho_1^k}$ 二阶矩修正$\\hat{M}^k=\\frac{M^k}{1-\\rho_2^k}$ $x^{k+1}=x^k-\\frac{\\eta}{\\sqrt{\\hat{M}^k+\\varepsilon 1_n}}\\circ \\hat{S}^k$ AdaBelief 修改二阶矩的计算 $Q^k=\\rho_2 Q^{k-1}+(1-\\rho_2)(g^k-S^k)\\circ (g^k-S^k)$ 修正二阶矩偏差时加入额外的$\\varepsilon$保证有下界 $\\hat{Q}^k=\\frac{Q^k+\\varepsilon}{1-\\rho_2^k}$ AdaBelief 算法在“大梯度，小曲率”情况下有优势 ",
  "keywords": [
    "AI", "Math"
  ],
  "articleBody": "引言 经验风险$R_{emp}(f(u,x)=\\frac{1}{n}\\sum_{i=1}^n l(f(u_i,x),v_i)$ 期望风险(真实风险)：$R_{exp}(f(u, x)) = \\mathbb{E}[l(f(u, x), v)]$ 结构风险模型：$R_{srm}\\frac{1}{n}\\sum_{i=1}^n l(f(u_i,x),v_i)+\\lambda J(f)$ 全体数据集最好算法$f^$，有限样本有限算法集最佳算法$\\hat{h}_H$，全体数据有限算法最佳$h_H^$ 近似误差$R_{exp}(h_H^)-R^$，估算误差$R_{emp}(\\hat{h}H) − R{exp}(h^∗_H)$ 最优化基础 广义实值函数 基本概念 广义实值函数：映射$\\mathbb{R}^n$-\u003e广义函数空间$\\mathbb{R}\\cup{\\pm\\infty}$ $\\alpha$-下水平集：$C_\\alpha={x|f(x)\\le\\alpha}$，上方图$\\mathrm{epi}$ $f = { (x, t) ∈ R^{n+1} |f(x) ≤ t}$ $\\alpha$-下水平集是闭集\u003c=\u003e下半连续\u003c=\u003e闭函数（上方图是闭集） 对偶范数$||y||*=sup{||x||\\le1}x^Ty$ 梯度$\\nabla f(x)=[\\frac{\\partial f}{\\partial x_1}(x),…,\\frac{\\partial f}{\\partial x_n}(x)]^T$，Hessian矩阵（$n\\times n$）:$\\nabla^2 f(x)$ 方向导数$\\partial f(x;d)=\\frac{\\partial d}{\\partial d}(x)=\\lim_{\\theta\\rightarrow 0 }\\frac{f(x+\\theta d)-f(x)}{\\theta}=\\nabla f(x)^T d$ 二阶方向导数$d^T\\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\\frac{\\partial f_i}{\\partial x_j}(x)$ 泰勒展开式：$f(x+d)=f(x)+\\nabla f(x+td)^T d=f(x)+\\nabla f(x)^T d+\\frac{1}{2}d^T\\nabla^2 f(x+td)d$ 凸性 凸集：$\\eta x_1+(1-\\eta) x_2\\in S$ 凸函数$f(\\eta x_1 +(1-\\eta)x_2 \\le \\eta f(x_1)+(1-\\eta)f(x_2)$\u003c=\u003e$f(y)\\ge f(x)+\\nabla f(x)^T(y-x)$\u003c=\u003e当且仅当在任意直线上是凸的 强凸：$\\exists \\mu\u003e0, f(y)\\ge f(x)+\\nabla f(x)^T(y-x)+\\frac{1}{2}\\mu ||x_2-x_1||_2^2$ 二阶条件$\\nabla ^2 f(x)\\ge 0$ 利普希茨连续 存在$L$，对于任意的$x,y\\in \\mathrm{dom} f$有：$||\\nabla f(x)-\\nabla f(y)|\\le L||x-y|||$\u003c=\u003e$||\\nabla ^2 f(x)||\\le L, \\forall x$ 凸函数，满足利普希茨条件，则$||\\nabla f(x)-\\nabla f(y)||^2\\le L(x-y)^T(\\nabla f(x)-\\nabla f(y))$ #三个等价条件 次梯度 $f(y)\\ge f(x)+g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\\partial f(x)$ 共轭函数 $f^*(y)=sup_x{y^Tx-f(x)}$ 性质：$f(x)+f*(y)\\ge x^Ty$，若$f$为闭函数，$f^{**}=f$ 优化算法与基本结构 算法基本结构 全局最小点$f(x^)",
  "wordCount" : "824",
  "inLanguage": "en",
  "datePublished": "2024-10-19T16:34:43+08:00",
  "dateModified": "2024-10-19T16:34:43+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sjj1017.github.io/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jiajun, Shen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sjj1017.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://sjj1017.github.io/" accesskey="h" title="Jiajun, Shen (Alt + H)">Jiajun, Shen</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://sjj1017.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/findout/" title="Findout">
                    <span>Findout</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      人工智能的数学基础
    </h1>
    <div class="post-meta"><span title='2024-10-19 16:34:43 +0800 CST'>October 19, 2024</span>&nbsp;·&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%bc%95%e8%a8%80" aria-label="引言">引言</a></li>
                <li>
                    <a href="#%e6%9c%80%e4%bc%98%e5%8c%96%e5%9f%ba%e7%a1%80" aria-label="最优化基础">最优化基础</a><ul>
                        
                <li>
                    <a href="#%e5%b9%bf%e4%b9%89%e5%ae%9e%e5%80%bc%e5%87%bd%e6%95%b0" aria-label="广义实值函数">广义实值函数</a></li>
                <li>
                    <a href="#%e5%88%a9%e6%99%ae%e5%b8%8c%e8%8c%a8%e8%bf%9e%e7%bb%ad" aria-label="利普希茨连续">利普希茨连续</a></li>
                <li>
                    <a href="#%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95%e4%b8%8e%e5%9f%ba%e6%9c%ac%e7%bb%93%e6%9e%84" aria-label="优化算法与基本结构">优化算法与基本结构</a></li>
                <li>
                    <a href="#%e7%ba%bf%e6%90%9c%e7%b4%a2%e6%8a%80%e6%9c%af" aria-label="线搜索技术">线搜索技术</a></li>
                <li>
                    <a href="#%e6%9c%80%e4%bc%98%e5%8c%96%e5%88%86%e6%94%af" aria-label="最优化分支">最优化分支</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%9c%80%e4%bc%98%e5%8c%96%e7%90%86%e8%ae%ba" aria-label="最优化理论">最优化理论</a></li>
                <li>
                    <a href="#%e6%97%a0%e7%ba%a6%e6%9d%9f%e5%8f%af%e5%be%ae%e4%bc%98%e5%8c%96%e9%97%ae%e9%a2%98" aria-label="无约束可微优化问题">无约束可微优化问题</a><ul>
                        
                <li>
                    <a href="#%e6%a2%af%e5%ba%a6%e7%b1%bb%e7%ae%97%e6%b3%95" aria-label="梯度类算法">梯度类算法</a></li>
                <li>
                    <a href="#%e6%ac%a1%e6%a2%af%e5%ba%a6%e6%b3%95" aria-label="次梯度法">次梯度法</a></li>
                <li>
                    <a href="#%e7%bb%8f%e5%85%b8%e7%89%9b%e9%a1%bf%e6%b3%95" aria-label="经典牛顿法">经典牛顿法</a></li>
                <li>
                    <a href="#%e4%bf%ae%e6%ad%a3%e7%89%9b%e9%a1%bf%e6%b3%95" aria-label="修正牛顿法">修正牛顿法</a></li>
                <li>
                    <a href="#%e9%9d%9e%e7%b2%be%e7%a1%ae%e7%89%9b%e9%a1%bf%e6%b3%95" aria-label="非精确牛顿法">非精确牛顿法</a></li>
                <li>
                    <a href="#%e6%8b%9f%e7%89%9b%e9%a1%bf%e6%9d%a1%e4%bb%b6" aria-label="拟牛顿条件">拟牛顿条件</a></li></ul>
                </li>
                <li>
                    <a href="#%e7%ba%a6%e6%9d%9f%e4%bc%98%e5%8c%96%e6%9c%80%e4%bc%98%e6%80%a7%e7%90%86%e8%ae%ba" aria-label="约束优化最优性理论">约束优化最优性理论</a></li>
                <li>
                    <a href="#%e7%ba%a6%e6%9d%9f%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95" aria-label="约束优化方法">约束优化方法</a><ul>
                        
                <li>
                    <a href="#%e4%ba%8c%e6%ac%a1%e7%bd%9a%e5%87%bd%e6%95%b0%e6%b3%95" aria-label="二次罚函数法">二次罚函数法</a></li>
                <li>
                    <a href="#%e5%86%85%e7%82%b9%e7%bd%9a%e5%87%bd%e6%95%b0%e5%b8%b8%e7%94%a8%e5%af%b9%e6%95%b0" aria-label="内点罚函数（常用对数）">内点罚函数（常用对数）</a></li>
                <li>
                    <a href="#%e7%b2%be%e7%a1%ae%e7%bd%9a%e5%87%bd%e6%95%b0%e6%b3%95" aria-label="精确罚函数法">精确罚函数法</a></li>
                <li>
                    <a href="#%e5%a2%9e%e5%b9%bf%e6%8b%89%e6%a0%bc%e6%9c%97%e6%97%a5%e5%87%bd%e6%95%b0%e6%b3%95" aria-label="增广拉格朗日函数法">增广拉格朗日函数法</a></li>
                <li>
                    <a href="#%e4%ba%a4%e6%9b%bf%e6%96%b9%e5%90%91%e4%b9%98%e5%ad%90%e6%b3%95admm" aria-label="交替方向乘子法ADMM">交替方向乘子法ADMM</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e4%b8%80%e9%98%b6%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95" aria-label="随机一阶优化方法">随机一阶优化方法</a><ul>
                        
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e7%b1%bb%e7%ae%97%e6%b3%95" aria-label="随机梯度类算法">随机梯度类算法</a></li>
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e5%8a%a8%e9%87%8f%e6%b3%95" aria-label="随机动量法">随机动量法</a></li>
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e6%ac%a1%e6%a2%af%e5%ba%a6%e6%b3%95" aria-label="随机次梯度法">随机次梯度法</a></li>
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e6%96%b9%e5%b7%ae%e7%bc%a9%e5%87%8f%e7%b1%bb%e6%96%b9%e6%b3%95" aria-label="随机方差缩减类方法">随机方差缩减类方法</a></li>
                <li>
                    <a href="#%e5%8a%a8%e6%80%81%e6%8a%bd%e6%a0%b7%e6%96%b9%e6%b3%95" aria-label="动态抽样方法">动态抽样方法</a></li>
                <li>
                    <a href="#sag%e7%ae%97%e6%b3%95" aria-label="SAG算法">SAG算法</a></li>
                <li>
                    <a href="#saga%e7%ae%97%e6%b3%95" aria-label="SAGA算法">SAGA算法</a></li>
                <li>
                    <a href="#svrg%e7%ae%97%e6%b3%95" aria-label="SVRG算法">SVRG算法</a></li>
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e9%80%92%e5%bd%92%e6%a2%af%e5%ba%a6%e6%b3%95sarah" aria-label="随机递归梯度法SARAH">随机递归梯度法SARAH</a></li>
                <li>
                    <a href="#%e5%b8%a6bb%e6%ad%a5%e9%95%bf%e7%9a%84%e6%96%b9%e5%b7%ae%e7%bc%a9%e5%87%8f%e7%b1%bb" aria-label="带BB步长的方差缩减类">带BB步长的方差缩减类</a></li>
                <li>
                    <a href="#adagrad" aria-label="AdaGrad">AdaGrad</a></li>
                <li>
                    <a href="#rmsprop" aria-label="RMSProp">RMSProp</a></li>
                <li>
                    <a href="#adam" aria-label="Adam">Adam</a></li>
                <li>
                    <a href="#adabelief" aria-label="AdaBelief">AdaBelief</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="引言">引言<a hidden class="anchor" aria-hidden="true" href="#引言">#</a></h3>
<ul>
<li>经验风险$R_{emp}(f(u,x)=\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)$</li>
<li>期望风险(真实风险)：$R_{exp}(f(u, x)) = \mathbb{E}[l(f(u, x), v)]$</li>
<li>结构风险模型：$R_{srm}\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)+\lambda J(f)$</li>
<li>全体数据集最好算法$f^<em>$，有限样本有限算法集最佳算法$\hat{h}_H$，全体数据有限算法最佳$h_H^</em>$</li>
<li>近似误差$R_{exp}(h_H^<em>)-R^</em>$，估算误差$R_{emp}(\hat{h}<em>H) − R</em>{exp}(h^∗_H)$</li>
<li>
<h3 id="最优化基础">最优化基础<a hidden class="anchor" aria-hidden="true" href="#最优化基础">#</a></h3>
</li>
<li>
<h4 id="广义实值函数">广义实值函数<a hidden class="anchor" aria-hidden="true" href="#广义实值函数">#</a></h4>
<ul>
<li>基本概念
<ul>
<li>广义实值函数：映射$\mathbb{R}^n$-&gt;广义函数空间$\mathbb{R}\cup{\pm\infty}$</li>
<li>$\alpha$-下水平集：$C_\alpha={x|f(x)\le\alpha}$，上方图$\mathrm{epi}$ $f = { (x, t) ∈ R^{n+1} |f(x) ≤ t}$</li>
<li>$\alpha$-下水平集是闭集&lt;=&gt;下半连续&lt;=&gt;闭函数（上方图是闭集）</li>
<li>对偶范数$||y||<em>*=sup</em>{||x||\le1}x^Ty$</li>
<li>梯度$\nabla f(x)=[\frac{\partial f}{\partial x_1}(x),&hellip;,\frac{\partial f}{\partial x_n}(x)]^T$，Hessian矩阵（$n\times n$）:$\nabla^2 f(x)$</li>
<li>方向导数$\partial f(x;d)=\frac{\partial d}{\partial d}(x)=\lim_{\theta\rightarrow 0 }\frac{f(x+\theta d)-f(x)}{\theta}=\nabla f(x)^T d$</li>
<li>二阶方向导数$d^T\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\frac{\partial f_i}{\partial x_j}(x)$</li>
<li>泰勒展开式：$f(x+d)=f(x)+\nabla f(x+td)^T d=f(x)+\nabla f(x)^T d+\frac{1}{2}d^T\nabla^2 f(x+td)d$</li>
<li>凸性
<ul>
<li>凸集：$\eta x_1+(1-\eta) x_2\in S$</li>
<li>凸函数$f(\eta x_1 +(1-\eta)x_2 \le \eta f(x_1)+(1-\eta)f(x_2)$&lt;=&gt;$f(y)\ge f(x)+\nabla f(x)^T(y-x)$&lt;=&gt;当且仅当在任意直线上是凸的</li>
<li>强凸：$\exists \mu&gt;0, f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}\mu ||x_2-x_1||_2^2$</li>
<li>二阶条件$\nabla ^2 f(x)\ge 0$</li>
</ul>
</li>
<li>
<h4 id="利普希茨连续">利普希茨连续<a hidden class="anchor" aria-hidden="true" href="#利普希茨连续">#</a></h4>
<ul>
<li>存在$L$，对于任意的$x,y\in \mathrm{dom} f$有：$||\nabla f(x)-\nabla f(y)|\le L||x-y|||$&lt;=&gt;$||\nabla ^2 f(x)||\le L, \forall x$</li>
<li>凸函数，满足利普希茨条件，则$||\nabla f(x)-\nabla f(y)||^2\le L(x-y)^T(\nabla f(x)-\nabla f(y))$</li>
<li>#三个等价条件</li>
</ul>
</li>
<li>次梯度
<ul>
<li>$f(y)\ge f(x)+g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\partial f(x)$</li>
</ul>
</li>
<li>共轭函数
<ul>
<li>$f^*(y)=sup_x{y^Tx-f(x)}$
<ul>
<li>性质：$f(x)+f*(y)\ge x^Ty$，若$f$为闭函数，$f^{**}=f$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="优化算法与基本结构">优化算法与基本结构<a hidden class="anchor" aria-hidden="true" href="#优化算法与基本结构">#</a></h4>
<ul>
<li>算法基本结构
<ul>
<li>全局最小点$f(x^<em>)&lt;f(x)$、严格全局最小点$x^</em>\ne x$</li>
<li>线搜索算法:</li>
<li>给定初始点x0∈R，置k:=0
若在 x[k] 点终止准则成立，则 x[k] 即为求得的最优解，终止; 否则，转步 3
根据方向计算规则，求得 x[k] 点搜索方向 d[k]
根据步长计算规则，求得搜索步长 η[k]
令x[k+1]=x[k]+η[k]*d[k]，置k:=k+1，转步2</li>
<li>终止准则：$||g^k||\le \varepsilon$或$||x^{k+1}-x^k||&lt;\varepsilon$或$||f(x^{k+1})-f(x^k)||&lt;\varepsilon$</li>
</ul>
</li>
<li>收敛速度
<ul>
<li>若$\lim \frac{||x^{k+1}-x^<em>||}{||x^k-x^</em>||}=\beta$，$0=\beta$超线性收敛，$0&lt;\beta&lt;1$线性收敛，$\beta=1$次线性收敛</li>
<li>二次收敛$\lim \frac{||x^{k+1}-x^<em>||}{||x^k-x^</em>||^2}=\beta$(任意常数)</li>
<li>存在$\alpha\ge 1,\beta &gt;0$，当$k$足够大（与$\alpha \beta$无关），恒有$||x^{k+1}-x^<em>||\le \beta ||x^k-x^</em>||^\alpha$</li>
<li>如果他对于任意正定二次函数，从任意初始点出发，可以经有限步迭代求得极小点，我们就称该算法具有二次终止性</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="线搜索技术">线搜索技术<a hidden class="anchor" aria-hidden="true" href="#线搜索技术">#</a></h4>
<ul>
<li>精确线搜索法
<ul>
<li>Armojo准则：$d^k$是$x^k处$的下降方向，若$f(x^k+\eta d^k)\le f(x^k)+\rho \eta \nabla f(x^k)^T d^k$，则$\eta$满足Armijo准则</li>
<li>Armijo线搜索算法
<ul>
<li>选择初始步长 η，参数 ρ,γ ∈ (0,1)，初始化 η ← ηˆ
若 ηk 满足Armijo准则，则终止计算，得步长 ηk. 否则，转步
令ηk :=γηk，转步2.</li>
</ul>
</li>
<li>Goldstein准则：在Armijo准则基础上加上$f(x^k+\eta d^k)\ge f(x^k)+(1-\rho) \eta \nabla f(x^k)^T d^k$</li>
</ul>
</li>
<li>非精确线搜索
<ul>
<li>Wolfe 准则，它的核心思想有两个：目标函数值应该有足够的下降；可接受点处的切线斜率 ≥ 初始斜率的 σ 倍</li>
<li>在Armijo准则上加伤$\nabla f(x^k+\eta d^k)^T d^k\ge \sigma \nabla f(x^k)^T d^k$</li>
<li>非精确线搜索步长的存在性：$f(x^k + ηd^k)$ 在 $η &gt; 0$ 时有下界，且 $∇f(x^k)^Td^k &lt; 0$</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="最优化分支">最优化分支<a hidden class="anchor" aria-hidden="true" href="#最优化分支">#</a></h4>
<ul>
<li>线性与非线性规划
<ul>
<li>线性规划LP：在线性等式和不等式约束下最优化一个线性目标函数</li>
<li>如果约束和目标函数中有一个非线性的，则问题就称为非线性规划问题</li>
</ul>
</li>
<li>二次规划QP
<ul>
<li>目标函数是变量的二次函数</li>
<li>Q半正定时QP是凸优化问题，可以用内点法在多项式时间内求解</li>
</ul>
</li>
<li>锥优化CO
<ul>
<li>非负性条件 $x ≥ 0$ 用锥包含约束替换后得到的优化问题</li>
<li>二阶锥$x_1^2 ⩾ x_2^2 +···+x^2_n,x_1 ⩾ 0$</li>
<li>对称半正定锥 $X=X^T$半正定</li>
</ul>
</li>
<li>整数规划ILP
<ul>
<li>部分或全部变量取整数的优化问题</li>
<li>0-1规划</li>
<li>混合整数规划：既有连续变量又有整数约束变量时，问题称为混合整数线性规划</li>
</ul>
</li>
<li>动态规划
<ul>
<li>涉及递推关系的计算方法，把问题分成阶段以便进行递推优化</li>
</ul>
</li>
</ul>
</li>
<li></li>
<li>
<h3 id="最优化理论">最优化理论<a hidden class="anchor" aria-hidden="true" href="#最优化理论">#</a></h3>
</li>
<li>Weierstrass 定理：条件任意成立一个：$\mathrm{dom f}$有界；存在常数$\bar{gamma}$使得下水平集$C_\gamma$是非空且有界的；$f$是强制的，即对于任意满足极限为$+\infty$的点列都有其函数值趋向于$+\infty$，则最优化问题的最小点集是非空且紧的</li>
<li>
<h3 id="无约束可微优化问题">无约束可微优化问题<a hidden class="anchor" aria-hidden="true" href="#无约束可微优化问题">#</a></h3>
</li>
<li>下降方向：如果存在$d$满足$\nabla f(x)^Td&lt;0$则$d$为一个下降方向。局部最优点处不能有下降方向。局部极小点$x^<em>$满足$\nabla f(x^</em>)=0$(一阶必要条件)，同时$\nabla^2f(x^*)$半正定（二阶必要条件），如果二阶连续可微，那么二阶必要条件是充分条件。</li>
<li>假设$f$#适当 且凸，则$x^<em>$是局部极小点&lt;=&gt;$0\in \partial f(x^</em>)$</li>
<li>对于二阶连续可微的目标函数，梯度法、牛顿法、拟牛顿法在每一次迭代均能看做是构建局部的二次模型，梯度法可以看做利用 $(1/η^k)I$作为Hessian矩阵估计，牛顿类算法利用真实Hessian矩阵，拟牛顿利用真实Hessian矩阵或逆的估计构建模型。牛顿法收敛最快计算量存储量大，梯度法相对最慢。</li>
<li>
<h4 id="梯度类算法">梯度类算法<a hidden class="anchor" aria-hidden="true" href="#梯度类算法">#</a></h4>
<ul>
<li>一般形式：$x^{k+1}=x^k+\eta_k d^k$，收敛速度：$L$-利普希茨连续时$0&lt;\eta&lt;\frac{1}{L}$时为$O(1/k)$，对强凸函数$0&lt;\eta&lt;\frac{1}{L+\eta}$时Q-线性收敛</li>
<li>精确线搜索、数值线性搜索法</li>
<li>BB方法：
<ul>
<li>选取$min||\eta y^{k-1}-s^{k-1}||^2$或$min|| y^{k-1}-\eta^{-1}s^{k-1}||^2$的解</li>
<li>$s^{k-1}=x^{k+1}-x^k$，$y^{k-1}=\nabla f(x^{k+1})-\nabla f(x^k)$</li>
<li>解分别为$\eta_{BB1}^k=\frac{(s^{k-1})^Ty^{k-1}} {(y^{k-1})^Ty^{k-1}}$，$\eta_{BB2}^k=\frac{(s^{k-1})^Ts^{k-1}} {(s^{k-1})^Ty^{k-1}}$</li>
<li>通过$η_m ⩽η_k ⩽η_M$截断过大或过小的步长，也可以使用两种步长的凸组合</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="次梯度法">次梯度法<a hidden class="anchor" aria-hidden="true" href="#次梯度法">#</a></h4>
<ul>
<li>迭代格式：$x^{k+1} = x^k − η^kg^k, g^k ∈ ∂f(x^k)$</li>
<li>若 $0 \notin ∂f(x)$，那么对于任意 $x^∗ ∈ argmin_x f(x)$和任意 $g ∈ ∂f(x)$，存在步长 $η &gt; 0$ 使得$||x−ηg−x^<em>||_2^2 &lt;||x−x^</em>||_2^2$</li>
<li>若至少存在一个极小点且次梯度有界，则$\sum \eta_k(f(x^k)-f(x^<em>))\le \frac{1}{2}||x^0-x^</em>||^2+\frac{1}{2}\sum \eta_k^2 M^2$</li>
</ul>
</li>
<li>
<h4 id="经典牛顿法">经典牛顿法<a hidden class="anchor" aria-hidden="true" href="#经典牛顿法">#</a></h4>
<ul>
<li>迭代格式：$x^{k+1} = x^k − \nabla^2f(x^k)^{-1}\nabla f(x^k), g^k ∈ ∂f(x^k)$</li>
<li>极小点处梯度为0，Hessian矩阵正定，则起始点足够近时，收敛是Q-二次的且梯度的范数Q-二次收敛到0</li>
</ul>
</li>
<li>
<h4 id="修正牛顿法">修正牛顿法<a hidden class="anchor" aria-hidden="true" href="#修正牛顿法">#</a></h4>
<ul>
<li>迭代格式：$x^{k+1} = x^k +\eta_k d^k$</li>
<li>确定矩阵$E^k$使得$\nabla ^2 f(x^k)+E^k$正定且条件数较小，求解$B^kd^k=-\nabla f(x^k)$，确定步长迭代。</li>
</ul>
</li>
<li>
<h4 id="非精确牛顿法">非精确牛顿法<a hidden class="anchor" aria-hidden="true" href="#非精确牛顿法">#</a></h4>
<ul>
<li>引入残差$r^k=\nabla^2 f(x^k)d^k+\nabla f(x^k)$，$||r^k||\le \alpha_k||\nabla f(x^k)||$</li>
<li>若存在$t&lt;1$使得$0&lt;\alpha_k&lt;t$则Q-线性收敛；若$\alpha_k$收敛到0，则Q-超线性收敛；若$\alpha_k=O(||\nabla f(x^k)||)$，则Q-二次收敛</li>
</ul>
</li>
<li>
<h4 id="拟牛顿条件">拟牛顿条件<a hidden class="anchor" aria-hidden="true" href="#拟牛顿条件">#</a></h4>
<ul>
<li>Hessian的近似矩阵满足$y^k=B^{k+1}s^k$，逆矩阵$s^k=H^{k+1}y^k$</li>
<li>迭代格式：$x^{k+1}=x^k+\alpha_k d^k$，$d^k=-(B^k)^{-1}\nabla f(x^k)=-H^k\nabla f(x^k)$</li>
<li>SR1秩一更新
<ul>
<li>$B^{k+1}=B^k+\frac{(y^k-B^ks^k)(y^k-B^ks^k)^T}{(y^k-B^ks^k)^T s^k}$</li>
<li>$H^{k+1}=H^k+\frac{(s^k-H^ky^k)(s^k-H^ky^k)^T}{(s^k-H^ky^k)^T y^k}$</li>
</ul>
</li>
<li>秩二更新
<ul>
<li>BFGS(相当于在满足割线方程的<strong>对称矩阵</strong>中找到离 $H^k$ 最近的矩阵)
<ul>
<li>利用割线方程$Ws^k=y^k$</li>
<li>$B^{k+1}=B^k+\frac{y^k(y^k)^T}{(s^k)^T y^k}-\frac{B^k s^k(B^ks^k)^T}{(s^k)^T B^ks^k}$</li>
<li>$H^{k+1}=(I-\rho_k y^k(s^k)^T)^TH^{k}(I-\rho_k y^k(s^k)^T)+\rho_ks^k(s^k)^T, \rho=\frac{1}{s^T y}$</li>
</ul>
</li>
<li>DFP方法，和BFGS为对偶关系
<ul>
<li>$Wy^k=s^k$</li>
</ul>
</li>
</ul>
</li>
<li>收敛性质
<ul>
<li>Zoutendijk 条件：满足Wolfe准则的一般迭代格式，有下界、连续可微、梯度利普希茨连续，则$\sum_{k=0}^\infty \cos^2(\theta_k)||\nabla f(x^k)||^2&lt;\infty$，$\cos\theta_k=\frac{-\nabla f(x^k)^T d^k}{||\nabla f(x^k)^T ||||d^k||}$</li>
<li>BFGS 全局收敛性：初始矩阵$B^0$对称正定，目标函数连续可微，对$f(x^0)$下水平集凸，且存在正数$m$以及$M$对任意$x,z$有$m||z||^2\le z^T \nabla ^2 f(x)z \le M||z||^2$，则 BFGS 格式结合 Wolfe 线搜索的拟牛顿算法全局收敛到极小值点</li>
<li>BFGS 收敛速度：目标二阶连续可微，最优点邻域Hessian矩阵利普希茨连续，BFGS收敛，误差之和小于正无穷，则Q-超线性收敛</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="约束优化最优性理论">约束优化最优性理论<a hidden class="anchor" aria-hidden="true" href="#约束优化最优性理论">#</a></h3>
</li>
<li>拉格朗日函数$L(x,\lambda,\nu)=f(x)+\sum_{i\in I} \lambda_i c_i(x)+\sum_{i \in E} \nu_i c_i(x)$</li>
<li>对偶函数$g(\lambda, \nu)=\inf_x L(x,\lambda,\nu)$是凸函数，给出原优化问题的下界$g(\lambda,\nu)\le p^*$</li>
<li>最优下界$\max g(\lambda,\nu)=max_{\lambda\ge 0,v}\inf_x L(x,\lambda,\nu)$
<ul>
<li>$domg = {(λ,ν) | λ ≥ 0,g(λ,ν) &gt; −∞}$，当 $(λ, ν) ∈ \mathrm{dom}  g$ 时，称为对偶可行解，对偶问题的最优值为 $q^∗$.称 $p^∗ − q^∗(≥ 0)$ 为对偶间隙，对偶间隙为零，则强对偶原理成立</li>
</ul>
</li>
<li>拉格朗日函数不动点$\nabla_x L(x^<em>,\lambda_1^</em>)=0$是必需但不充分的</li>
<li>某点$x^<em>$不存在一阶可行下降方向时，$\nabla_x L(x^</em>,\lambda_1^<em>)=0,\lambda_1^</em>\ge 0$且(互补松弛条件：)$\lambda_1^<em>c_1(x^</em>)=0$</li>
<li>切锥$T_X(x)$：切向量$d=\lim_{k\rightarrow \infty}\frac{z_k-x}{t_k}$的集合，最优化要求切锥(可行方向集合)不包含使得目标函数值下降的方向</li>
<li>几何最优性条件：对局部极小点的可行点，目标和约束函数可微，则$d^T\nabla f(x^<em>)\ge 0, \forall d \in T_X(x^</em>)$&lt;=&gt;$T_X(x^<em>)\cap{d|\nabla f(x^</em>)^T d&lt;0}=\varnothing$</li>
<li>线性化可行锥：$F(x)={d|d^T∇c_i(x) = 0, ∀ i ∈ E； d^T∇c_i(x)≤0,∀i∈A(x)∩I}$，积极集$A(x)=E∪{i∈I : c_i(x)=0}$</li>
<li>线性无关约束规格：给定可行点 $x$ 及相应的积极集 $A(x)$. 如果积极集对应的约束函数的梯度, 即 $∇c_i(x), i ∈ A(x)$, 是线性无关的, 则称线性无关约束规格 (LICQ) 在点 $x$ 处成立，如果LICQ 成立，则有 $T_X (x) = F (x)$</li>
<li>MFCQ：如果存在一个向量 $w ∈ R^n$, 使得$∇c_i(x)^Tw &lt; 0, ∀i ∈ A(x) ∩ I;∇c_i(x)^Tw = 0, ∀i ∈ E$，并且等式约束对应的梯度集 ${∇c_i(x), i ∈ E}$是线性无关的，则称 MFCQ 在点 x 处成立</li>
<li>KKT条件：（如果局部极小点处有$T_X (x^∗) = F (x^∗)$）
<ul>
<li>稳定性条件$\nabla_x L(x^<em>,\lambda^</em>)=\nabla f(x^<em>)+\sum_{i\in I\cup E} \lambda_i^</em>\nabla c_i(x^*)=0$</li>
<li>原始可行性条件 $c_i (x^∗) = 0, ∀i ∈ E,$^</li>
<li>原始可行性条件 $c_i (x^∗) ⩽ 0, ∀i ∈ I$</li>
<li>对偶可行性条件 $λ^∗_i ⩾0,∀i∈I$</li>
<li>互补松弛条件 $λ^∗_i c_i (x^∗) = 0,∀i ∈ I$</li>
</ul>
</li>
<li>二阶最优性条件：
<ul>
<li>二阶必要条件：如果局部最优解处处有$T_X (x^∗) = F (x^∗)$，$(x^<em>,\lambda^</em>)$满足KKT条件，则$d^T∇^2_{xx}L(x^∗,λ^∗)d ⩾ 0, ∀d ∈ C (x^∗,λ^∗)$</li>
<li>二阶充分条件：$d^T∇^2_{xx}L(x^∗,λ^∗)d&gt;0, ∀d∈C(x^∗,λ^∗),d\ne0$，那么 $x^∗$ 为一个严格局部极小解.</li>
<li></li>
</ul>
</li>
<li>
<h3 id="约束优化方法">约束优化方法<a hidden class="anchor" aria-hidden="true" href="#约束优化方法">#</a></h3>
</li>
<li>
<h4 id="二次罚函数法">二次罚函数法<a hidden class="anchor" aria-hidden="true" href="#二次罚函数法">#</a></h4>
<ul>
<li>等式二次罚函数
<ul>
<li>$P_E(x,\sigma)=f(x)+\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)，\sigma&gt;0$</li>
<li>给定 σ1 &gt; 0,x0,k ← 1.罚因子增长系数 ρ &gt; 1;
while 未达到收敛准则 do
以 xk 为初始点，求解 x[k+1] = argmin PE (x, σk);
选取 σ[k+1] = ρ*σ[k];
k ← k + 1;
end</li>
<li>收敛性：
<ul>
<li>设 $x^{k+1}$ 是 $P_E (x, σ^k)$ 的全局极小解, $σ^k$ 单调上升趋于无穷, 则 $x^k$ 的每个极限点$x^∗$都是原问题的全局极小解</li>
<li>$\sigma c_i\rightarrow -\lambda_i^*$（一定条件下）</li>
</ul>
</li>
</ul>
</li>
<li>不等式二次罚函数
<ul>
<li>$P_I(x,\sigma)=f(x)+\frac{1}{2}\sigma \sum_{i\in I}\tilde{c}_i^2(x)，\sigma&gt;0, \tilde{c}_i(x)=\max{c_i(x),0}$</li>
</ul>
</li>
<li>一般约束的二次罚函数
<ul>
<li>$P(x,\sigma)=f(x)+\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}<em>i^2(x)+\sum</em>{i\in E}c_i^2(x))$</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="内点罚函数常用对数">内点罚函数（常用对数）<a hidden class="anchor" aria-hidden="true" href="#内点罚函数常用对数">#</a></h4>
<ul>
<li>$P_I(x,\sigma)=f(x)-\sigma \sum_{i\in I}\ln(-c_i(x))$( 罚因子逐渐缩小，系数$\rho$)</li>
<li>收敛性：$|\sigma_k\sum_{i \in I}(-c_i(x^{k+1})|\le \varepsilon$（实际上极限为0）</li>
</ul>
</li>
<li>
<h4 id="精确罚函数法">精确罚函数法<a hidden class="anchor" aria-hidden="true" href="#精确罚函数法">#</a></h4>
<ul>
<li>$l_1$<strong>罚函数</strong>：$P(x,\sigma)=f(x)+\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}<em>i(x)+\sum</em>{i\in E}|c_i(x)|)$</li>
<li>当罚因子充分大 $σ&gt;||λ^*||_∞$(不需要是正无穷) 时，原问题的极小值点就是罚函数的极小值点</li>
<li>
<h4 id="增广拉格朗日函数法">增广拉格朗日函数法<a hidden class="anchor" aria-hidden="true" href="#增广拉格朗日函数法">#</a></h4>
<ul>
<li>等式约束
<ul>
<li>增广拉格朗日函数$L_\sigma(x,\lambda)=f(x)+\sum_{i\in E}\lambda_i c_i(x)+\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)$</li>
<li>初始坐标、乘子、罚因子及其更新常数，约束违反常数，精度，迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 等式约束满足精度 then 返回近似解，终止
else 更新乘子、罚因子
end</li>
<li>罚因子更新$σ_{k+1} = ρσ{k}$，乘子更新$λ^{k+1}_i=λ^k_i+σ_i c_i (x^{k+1})$</li>
</ul>
</li>
<li>一般约束约束
<ul>
<li>引入松弛变量，$L(x,s,\lambda,\mu)=f(x)+\sum_{i\in E}\lambda_i c_i(x)+ \sum_{i\in I}\mu_i (c_i(x)+s_i)$，$s_i\ge 0$；$p(x,s)=\sum_{i\in E}c_i^2(x)+\sum_{i \in I}(c_i(x)+s_i)^2$</li>
<li>增广拉格朗日函数：$L_\sigma (x,s,\lambda,\mu)=L+p(x,s)$</li>
<li>取最优的$s_i=\max{-\frac{\mu_i}{\sigma_k}-c_i(x),0}$，原问题等价于优化$L_\sigma (x,\lambda,\mu)$</li>
<li>初始坐标、乘子、罚因子及其更新常数，约束违反常数e，精度，常数alpha和beta、迭代步数
for k=.. do
从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度
if 约束违反度小与ek then
if 约束违反度小于违反度常数e 且梯度范数小于精度 then
返回近似解，终止
else 更新两个乘子、罚因子不变，减小精度条件和约束违反度
else 乘子不变，更新罚因子，调整误差和约束违反度
end</li>
<li>乘子更新$E:\lambda_i^{k+1}=\lambda_i^k \sigma_k c_i(x^{k+1})$，$I:\mu_i^{k+1}=\max{\mu_i^k +\sigma_k c_i(x^{k+1}),0}$</li>
<li>误差和约束违反度：$\eta_{k+1}=\frac{\eta_k}{\sigma_{k+1}}，\varepsilon_{k+1}=\frac{\varepsilon_k}{\sigma_{k+1}^\beta}$或$\eta_{k+1}=\frac{1}{\sigma_{k+1}}，\varepsilon_{k+1}=\frac{1}{\sigma_{k+1}^\alpha}$</li>
</ul>
</li>
<li>凸优化问题</li>
</ul>
</li>
<li>
<h4 id="交替方向乘子法admm">交替方向乘子法ADMM<a hidden class="anchor" aria-hidden="true" href="#交替方向乘子法admm">#</a></h4>
<ul>
<li>对于优化$f_1(x)+f_2(x), A_1x_1+A_2x_2=b$，</li>
<li>增广拉格朗日函数$L_\rho(x_1,x_2,y)=f_1(x_1)+f_2(x_2)+y^T(A_1x_1+A_2x_2-b)+\frac{\rho}{2}||A_1x_1+A_2x_2-b||^2_2$，</li>
<li>[[乘子更新]]$y^{k+1}=y^k+\tau\rho (A_1x_1^{k+1}+A_2x_2^{k+1}-b)$</li>
<li>交替求极小：$x_1^{k+1}=\argmin L_\rho (x_1,x_2^k,y^k)$；$x_2^{k+1}=\argmin L_\rho (x_1^{k+1},x_2,y^k)$；[[乘子更新]]</li>
</ul>
</li>
</ul>
</li>
<li>
<h3 id="随机一阶优化方法">随机一阶优化方法<a hidden class="anchor" aria-hidden="true" href="#随机一阶优化方法">#</a></h3>
</li>
<li>
<h4 id="随机梯度类算法">随机梯度类算法<a hidden class="anchor" aria-hidden="true" href="#随机梯度类算法">#</a></h4>
<ul>
<li>随机梯度法
<ul>
<li>迭代格式$x^{k+1}=x^k-\eta_k \nabla f_{ik}(x^k)$</li>
</ul>
</li>
<li>小批量随机梯度法
<ul>
<li>迭代格式$x^{k+1}=x^k-\eta\nabla f _{S_k}(x^k), \nabla f <em>{S_k}(x^k)=\frac{1}{|S_k|}\sum</em>{i\in S_k }\nabla f_i(x^k)$</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="随机动量法">随机动量法<a hidden class="anchor" aria-hidden="true" href="#随机动量法">#</a></h4>
<ul>
<li>迭代格式：$v^k=\beta_k v^{k-1}+\nabla f_{i_k}(x^k), x^{k+1}=x^k-\eta_k v^k$</li>
<li>等价于重球法$x^{k+1}=x^k-\eta_k \nabla f_{i_k}(x^k)+\hat{\beta}_k(x^k-x^{k-1})$</li>
</ul>
</li>
<li>
<h4 id="随机次梯度法">随机次梯度法<a hidden class="anchor" aria-hidden="true" href="#随机次梯度法">#</a></h4>
<ul>
<li>迭代格式： $x^{k+1} = x^k − η_kg^k, g^k ∈ ∂f_{i_k} (x^k)$,</li>
<li>当满足$\sum \eta_k =+\infty, \frac{\sum_{1\sim K-1}}{\eta_k^2}{\sum_{1\sim K-1}}{\eta_k}\rightarrow 0$时算法收敛</li>
<li>函数的渐近表现很脆弱，这种算法结构很难实现并行化，当问题规模较大时，算法执行时间长</li>
</ul>
</li>
<li>
<h4 id="随机方差缩减类方法">随机方差缩减类方法<a hidden class="anchor" aria-hidden="true" href="#随机方差缩减类方法">#</a></h4>
<ul>
<li>$SGD_<em>$：$x^{k+1}=x^k-\eta(\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^</em>))$</li>
<li>
<h4 id="动态抽样方法">动态抽样方法<a hidden class="anchor" aria-hidden="true" href="#动态抽样方法">#</a></h4>
<ul>
<li>范数测试、内积测试、锐角测试</li>
<li>分层抽样方法：将训练样本分类，每一类独立采样子集
<ul>
<li>$x^{k+1}=x^k-\frac{\eta_k}{n}\sum_{i=1}^{t}\frac{n_i^k}{b_i^k}\sum_{s\in B_i^k} \nabla f_s(x^k)$</li>
</ul>
</li>
</ul>
</li>
<li>
<h4 id="sag算法">SAG算法<a hidden class="anchor" aria-hidden="true" href="#sag算法">#</a></h4>
<ul>
<li>全梯度的估计$\bar{g}^k=\frac{1}{n}\sum_{j=1}^n v_j^k$</li>
<li>迭代时$v_j^{k+1}=\nabla f_{i_k}(x_k)\quad \mathrm{if} j=i_k ,\mathrm{else}: v_j^k$，即更新之后将抽取的样本对应的随机梯度改为当前的随机梯度值</li>
<li>由于每次只有一部分改变，可以写成$\bar{g}^k=\bar{g}^{k-1}-\frac{1}{n}v_{i_k}^{k-1}+\frac{1}{n}v_{i_k}^k$</li>
</ul>
</li>
<li>
<h4 id="saga算法">SAGA算法<a hidden class="anchor" aria-hidden="true" href="#saga算法">#</a></h4>
<ul>
<li>SAGA 算法选择一个参考点$\bar{x}^i,v_i=\nabla f_i(\bar{x}^i)$</li>
<li>$g^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(\bar{x}^{i_k})+\frac{1}{n}\sum_{j=1}^n \nabla f_j(\bar{x}_j)$</li>
<li>x</li>
<li>线性收敛速度</li>
</ul>
</li>
<li>
<h4 id="svrg算法">SVRG算法<a hidden class="anchor" aria-hidden="true" href="#svrg算法">#</a></h4>
<ul>
<li>每经过几次迭代之后设置检查点，计算全梯度作为参考
<ul>
<li>$\nabla f(x^j)=\frac{1}{n}\sum_{i=1}^n \nabla f_i(x^j)$</li>
<li>$v^k=\nabla f_{i_k}(x^k)-(\nabla f_{i_k}(x^j)-\nabla f(x^j))$</li>
</ul>
</li>
<li>对于参考点的函数值期望的意义下线性收敛速度</li>
</ul>
</li>
<li>
<h4 id="随机递归梯度法sarah">随机递归梯度法SARAH<a hidden class="anchor" aria-hidden="true" href="#随机递归梯度法sarah">#</a></h4>
<ul>
<li>梯度估计的更新$v^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^{k-1})+v^{k-1} , v^0=$全梯度</li>
<li>不是无偏估计</li>
</ul>
</li>
<li>
<h4 id="带bb步长的方差缩减类">带BB步长的方差缩减类<a hidden class="anchor" aria-hidden="true" href="#带bb步长的方差缩减类">#</a></h4>
</li>
<li>
<h4 id="adagrad">AdaGrad<a hidden class="anchor" aria-hidden="true" href="#adagrad">#</a></h4>
<ul>
<li>$x^{k+1}=x^k-\frac{\eta}{\sqrt{G^k+\varepsilon 1_n}}\circ g^k$</li>
<li>$G^{k+1}=G^k+g^k \circ g^k$</li>
</ul>
</li>
<li>
<h4 id="rmsprop">RMSProp<a hidden class="anchor" aria-hidden="true" href="#rmsprop">#</a></h4>
<ul>
<li>$x^{k+1}=x^k-\frac{\eta}{\sqrt{M^k+\varepsilon 1_n}}\circ g^k$</li>
<li>$G^{k+1}=\rho G^k+(1-\rho)g^{k+1} \circ g^{k+1}$</li>
</ul>
</li>
<li>
<h4 id="adam">Adam<a hidden class="anchor" aria-hidden="true" href="#adam">#</a></h4>
<ul>
<li>梯度$g^k=\nabla f_i (x^k)$</li>
<li>一阶矩$S^k=\rho_1 S^{k-1}+(1-\rho_1)g^k$</li>
<li>二阶矩$M^k=\rho_2 M^{k-1}+(1-\rho_2)g^k\circ g^k$</li>
<li>一阶矩修正$\hat{S}^k=\frac{S^k}{1-\rho_1^k}$</li>
<li>二阶矩修正$\hat{M}^k=\frac{M^k}{1-\rho_2^k}$</li>
<li>$x^{k+1}=x^k-\frac{\eta}{\sqrt{\hat{M}^k+\varepsilon 1_n}}\circ \hat{S}^k$</li>
</ul>
</li>
<li>
<h4 id="adabelief">AdaBelief<a hidden class="anchor" aria-hidden="true" href="#adabelief">#</a></h4>
<ul>
<li>修改二阶矩的计算
<ul>
<li>$Q^k=\rho_2 Q^{k-1}+(1-\rho_2)(g^k-S^k)\circ (g^k-S^k)$</li>
</ul>
</li>
<li>修正二阶矩偏差时加入额外的$\varepsilon$保证有下界
<ul>
<li>$\hat{Q}^k=\frac{Q^k+\varepsilon}{1-\rho_2^k}$</li>
</ul>
</li>
<li>AdaBelief 算法在“大梯度，小曲率”情况下有优势</li>
</ul>
</li>
</ul>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://sjj1017.github.io/tags/ai/">AI</a></li>
      <li><a href="https://sjj1017.github.io/tags/math/">Math</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://sjj1017.github.io/">Jiajun, Shen</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
