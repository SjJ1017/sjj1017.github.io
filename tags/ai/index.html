<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>AI | Jiajun, Shen</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://sjj1017.github.io/tags/ai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://sjj1017.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://sjj1017.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://sjj1017.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://sjj1017.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://sjj1017.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://sjj1017.github.io/tags/ai/index.xml">
<link rel="alternate" hreflang="en" href="https://sjj1017.github.io/tags/ai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="AI" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://sjj1017.github.io/tags/ai/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="AI"/>
<meta name="twitter:description" content=""/>

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://sjj1017.github.io/" accesskey="h" title="Jiajun, Shen (Alt + H)">Jiajun, Shen</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://sjj1017.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://sjj1017.github.io/findout/" title="Findout">
                    <span>Findout</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    AI
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论自动驾驶
    </h2>
  </header>
  <div class="entry-content">
    <p>引子 如何看待可解释性？何凯明在采访中说的这句话适合拿来当作一个很好的例子：
“你为什么信任人类司机呢？当你去打车的时候，其实司机知识一个陌生人，你根本不认识这个司机，你只知道司机是个人类，那你为什么还能相信这位司机呢？是因为你觉得他的大脑具有可解释性，还是因为你明白，正常来说，一个训练有素、经验丰富的人类司机，从经验主义的角度看是大概率能做好这件事的？”
这句话一半是完全对的，一半却是完全错的。
为什么可解释性不重要？ 可解释并无意义，它只是对于实在以及混乱无意识的一种幻想。这种幻想驱使我们不断去对未知的事物寻求解释，但一个残酷的事实是：寻求解释本身是一个符号化的过程，符号化必然伴随着剩余，必然会留下不可符号化的东西，他们继续在我们语言之外鬼鬼祟祟，但也正是这些语言结构所无能为力的地方，反过来又构建出我们自己。刘慈欣的小说《朝闻道》无意之间透露出我们与实在的那种关系：一旦我们知道了宇宙的终极奥秘，我们则会走向毁灭。因为符号化所带来的残余，或者说创伤，是构成我们主体的必要条件——人正是在这面残破的语言之镜里看到自己，进而认识到自己是一个统一的整体的。一旦我们的幻想能够成立，语言结构浩浩荡荡地，完美地实现了把整个实在界符号化的过程（而无一点剩余），那么这面镜子的映像则会与我们完全重合，我们也无法认识到自己的主体性。
可解释性既然对于终极真理的追求没有意义，但其对于人类“信任”的意义到这里还未被阐述。何凯明聪明地使用“经验主义”来顶替了可解释性在这里的意义，从而将可解释性从信任的必要条件中驱逐，然而他始终未提到构成信任的一个真正的必要条件：统一的秩序。
什么构成了信任？ 重回艾伦·坡的《被窃的信》，王后收到一封信，国王进来之后对信视而不见，而王后也无法把这封信收起来。经过的大臣在王后的眼下把这封信换成了另一封，虽然王后看得清清楚楚，却无法声张，因为她要维持住国王的“视而不见”，而不能打草惊蛇。在这个故事里，一切都达到了平衡，王后对国王不知道信的内容这个事实心存幻想，也正是这个幻想促使她不敢轻举妄动；而国王的凝视，正是大臣能在光天化日之下替换信件的原因；大臣最终没有将这封信拆开，继续保持了这个平衡。尽管这封信到最后仍然以一个戴着面具的形象出现，我们无从得知它的内容，但是它的存在构成了三个人的牵制。一个很普遍的看法是把这种三人的关系平移到故事里的杜宾、警长、大臣身上，以描述这种模式的重复。然而，如果固定住视角来考察这三个人，那么杜宾则是一个“闯入者”。信件的内容对他来说已无任何意义，也就是在这个条件下，侦探并没有进入被信件约束的这个秩序之中，正是因为不处于这个秩序，他才可以轻松地取回信件。
而信任则是一种平衡，这种平衡也需要在一种统一的秩序构建下找到自己的言语。想象一个普通司机驾驶的车有一定概率产生事故，而有一个驾驶技术高超的精神病司机，在正常时不可能发生事故，而以和普通司机车祸概率相同的概率发病，发病时则必会发生交通事故。在这个例子中，一个普通司机是一个更加值得信任的选择，因为精神病司机在发病的时候将会脱离这个我们使用道德、律法甚至一切语言所构建出来的秩序，而秩序之外的东西至少缺少了一个约束变量，因而也导致了不信任。
再次声明一点，我这里说的语言，指的是一切符号和规律结构的几何，而不仅仅是人类的语言，因为后者已经被人工智能所很好地掌握。我们至今仍然不能够信任人工智能，让它驾驶我们的汽车，因为在象征秩序的学习历程中，它还仅仅处在婴儿时期。我们在把实在符号化的过程中构建自身，也构建出这个象征秩序；而人工智能则以它自己的方式符号化我们的象征秩序，在这个过程中产生了我们象征秩序中的剩余，也在我们的秩序里发现了创伤。这种秩序与秩序间的迥异，最终导致了人工智能和人之间存在一种隔阂，换而言之，我们来自于不同的公司，又怎么能够互相信任呢。
在我们的秩序里的那些丑陋、审判以及口诛笔伐，在人工智能的语言模型里成为了不可以触及的红线，这构成了人工智能的创伤，也就是它的符号化过程所无能为力的部分。然而对我们来说，这些秩序的阴暗本身是秩序所带有的，也进而成为了秩序本身。因而人类与人工智能各自的象征秩序里必然存在着不同的创伤，因而也历经着完全不一样的秩序构建过程和主体构建过程。这种差别已经足以大到天翻地覆，而无法仅仅用“经验主义”缓解和消灭。
</p>
  </div>
  <footer class="entry-footer"><span title='2024-10-21 09:21:33 +0800 CST'>October 21, 2024</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论自动驾驶" href="https://sjj1017.github.io/posts/aboutself-drivingcar/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">人工智能的数学基础
    </h2>
  </header>
  <div class="entry-content">
    <p>引言 经验风险$R_{emp}(f(u,x)=\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)$ 期望风险(真实风险)：$R_{exp}(f(u, x)) = \mathbb{E}[l(f(u, x), v)]$ 结构风险模型：$R_{srm}\frac{1}{n}\sum_{i=1}^n l(f(u_i,x),v_i)&#43;\lambda J(f)$ 全体数据集最好算法$f^$，有限样本有限算法集最佳算法$\hat{h}_H$，全体数据有限算法最佳$h_H^$ 近似误差$R_{exp}(h_H^)-R^$，估算误差$R_{emp}(\hat{h}H) − R{exp}(h^∗_H)$ 最优化基础 广义实值函数 基本概念 广义实值函数：映射$\mathbb{R}^n$-&gt;广义函数空间$\mathbb{R}\cup{\pm\infty}$ $\alpha$-下水平集：$C_\alpha={x|f(x)\le\alpha}$，上方图$\mathrm{epi}$ $f = { (x, t) ∈ R^{n&#43;1} |f(x) ≤ t}$ $\alpha$-下水平集是闭集&lt;=&gt;下半连续&lt;=&gt;闭函数（上方图是闭集） 对偶范数$||y||*=sup{||x||\le1}x^Ty$ 梯度$\nabla f(x)=[\frac{\partial f}{\partial x_1}(x),…,\frac{\partial f}{\partial x_n}(x)]^T$，Hessian矩阵（$n\times n$）:$\nabla^2 f(x)$ 方向导数$\partial f(x;d)=\frac{\partial d}{\partial d}(x)=\lim_{\theta\rightarrow 0 }\frac{f(x&#43;\theta d)-f(x)}{\theta}=\nabla f(x)^T d$ 二阶方向导数$d^T\nabla^2 f(x)d$，Jacobi矩阵$[J(x)]_{ij}=\frac{\partial f_i}{\partial x_j}(x)$ 泰勒展开式：$f(x&#43;d)=f(x)&#43;\nabla f(x&#43;td)^T d=f(x)&#43;\nabla f(x)^T d&#43;\frac{1}{2}d^T\nabla^2 f(x&#43;td)d$ 凸性 凸集：$\eta x_1&#43;(1-\eta) x_2\in S$ 凸函数$f(\eta x_1 &#43;(1-\eta)x_2 \le \eta f(x_1)&#43;(1-\eta)f(x_2)$&lt;=&gt;$f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)$&lt;=&gt;当且仅当在任意直线上是凸的 强凸：$\exists \mu&gt;0, f(y)\ge f(x)&#43;\nabla f(x)^T(y-x)&#43;\frac{1}{2}\mu ||x_2-x_1||_2^2$ 二阶条件$\nabla ^2 f(x)\ge 0$ 利普希茨连续 存在$L$，对于任意的$x,y\in \mathrm{dom} f$有：$||\nabla f(x)-\nabla f(y)|\le L||x-y|||$&lt;=&gt;$||\nabla ^2 f(x)||\le L, \forall x$ 凸函数，满足利普希茨条件，则$||\nabla f(x)-\nabla f(y)||^2\le L(x-y)^T(\nabla f(x)-\nabla f(y))$ #三个等价条件 次梯度 $f(y)\ge f(x)&#43;g^T (y-x)$，称$g$为次梯度，次梯度的集合为次微分$\partial f(x)$ 共轭函数 $f^*(y)=sup_x{y^Tx-f(x)}$ 性质：$f(x)&#43;f*(y)\ge x^Ty$，若$f$为闭函数，$f^{**}=f$ 优化算法与基本结构 算法基本结构 全局最小点$f(x^)&lt;f(x)$、严格全局最小点$x^\ne x$ 线搜索算法: 给定初始点x0∈R，置k:=0 若在 x[k] 点终止准则成立，则 x[k] 即为求得的最优解，终止; 否则，转步 3 根据方向计算规则，求得 x[k] 点搜索方向 d[k] 根据步长计算规则，求得搜索步长 η[k] 令x[k&#43;1]=x[k]&#43;η[k]*d[k]，置k:=k&#43;1，转步2 终止准则：$||g^k||\le \varepsilon$或$||x^{k&#43;1}-x^k||&lt;\varepsilon$或$||f(x^{k&#43;1})-f(x^k)||&lt;\varepsilon$ 收敛速度 若$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||}=\beta$，$0=\beta$超线性收敛，$0&lt;\beta&lt;1$线性收敛，$\beta=1$次线性收敛 二次收敛$\lim \frac{||x^{k&#43;1}-x^||}{||x^k-x^||^2}=\beta$(任意常数) 存在$\alpha\ge 1,\beta &gt;0$，当$k$足够大（与$\alpha \beta$无关），恒有$||x^{k&#43;1}-x^||\le \beta ||x^k-x^||^\alpha$ 如果他对于任意正定二次函数，从任意初始点出发，可以经有限步迭代求得极小点，我们就称该算法具有二次终止性 线搜索技术 精确线搜索法 Armojo准则：$d^k$是$x^k处$的下降方向，若$f(x^k&#43;\eta d^k)\le f(x^k)&#43;\rho \eta \nabla f(x^k)^T d^k$，则$\eta$满足Armijo准则 Armijo线搜索算法 选择初始步长 η，参数 ρ,γ ∈ (0,1)，初始化 η ← ηˆ 若 ηk 满足Armijo准则，则终止计算，得步长 ηk. 否则，转步 令ηk :=γηk，转步2. Goldstein准则：在Armijo准则基础上加上$f(x^k&#43;\eta d^k)\ge f(x^k)&#43;(1-\rho) \eta \nabla f(x^k)^T d^k$ 非精确线搜索 Wolfe 准则，它的核心思想有两个：目标函数值应该有足够的下降；可接受点处的切线斜率 ≥ 初始斜率的 σ 倍 在Armijo准则上加伤$\nabla f(x^k&#43;\eta d^k)^T d^k\ge \sigma \nabla f(x^k)^T d^k$ 非精确线搜索步长的存在性：$f(x^k &#43; ηd^k)$ 在 $η &gt; 0$ 时有下界，且 $∇f(x^k)^Td^k &lt; 0$ 最优化分支 线性与非线性规划 线性规划LP：在线性等式和不等式约束下最优化一个线性目标函数 如果约束和目标函数中有一个非线性的，则问题就称为非线性规划问题 二次规划QP 目标函数是变量的二次函数 Q半正定时QP是凸优化问题，可以用内点法在多项式时间内求解 锥优化CO 非负性条件 $x ≥ 0$ 用锥包含约束替换后得到的优化问题 二阶锥$x_1^2 ⩾ x_2^2 &#43;···&#43;x^2_n,x_1 ⩾ 0$ 对称半正定锥 $X=X^T$半正定 整数规划ILP 部分或全部变量取整数的优化问题 0-1规划 混合整数规划：既有连续变量又有整数约束变量时，问题称为混合整数线性规划 动态规划 涉及递推关系的计算方法，把问题分成阶段以便进行递推优化 最优化理论 Weierstrass 定理：条件任意成立一个：$\mathrm{dom f}$有界；存在常数$\bar{gamma}$使得下水平集$C_\gamma$是非空且有界的；$f$是强制的，即对于任意满足极限为$&#43;\infty$的点列都有其函数值趋向于$&#43;\infty$，则最优化问题的最小点集是非空且紧的 无约束可微优化问题 下降方向：如果存在$d$满足$\nabla f(x)^Td&lt;0$则$d$为一个下降方向。局部最优点处不能有下降方向。局部极小点$x^$满足$\nabla f(x^)=0$(一阶必要条件)，同时$\nabla^2f(x^*)$半正定（二阶必要条件），如果二阶连续可微，那么二阶必要条件是充分条件。 假设$f$#适当 且凸，则$x^$是局部极小点&lt;=&gt;$0\in \partial f(x^)$ 对于二阶连续可微的目标函数，梯度法、牛顿法、拟牛顿法在每一次迭代均能看做是构建局部的二次模型，梯度法可以看做利用 $(1/η^k)I$作为Hessian矩阵估计，牛顿类算法利用真实Hessian矩阵，拟牛顿利用真实Hessian矩阵或逆的估计构建模型。牛顿法收敛最快计算量存储量大，梯度法相对最慢。 梯度类算法 一般形式：$x^{k&#43;1}=x^k&#43;\eta_k d^k$，收敛速度：$L$-利普希茨连续时$0&lt;\eta&lt;\frac{1}{L}$时为$O(1/k)$，对强凸函数$0&lt;\eta&lt;\frac{1}{L&#43;\eta}$时Q-线性收敛 精确线搜索、数值线性搜索法 BB方法： 选取$min||\eta y^{k-1}-s^{k-1}||^2$或$min|| y^{k-1}-\eta^{-1}s^{k-1}||^2$的解 $s^{k-1}=x^{k&#43;1}-x^k$，$y^{k-1}=\nabla f(x^{k&#43;1})-\nabla f(x^k)$ 解分别为$\eta_{BB1}^k=\frac{(s^{k-1})^Ty^{k-1}} {(y^{k-1})^Ty^{k-1}}$，$\eta_{BB2}^k=\frac{(s^{k-1})^Ts^{k-1}} {(s^{k-1})^Ty^{k-1}}$ 通过$η_m ⩽η_k ⩽η_M$截断过大或过小的步长，也可以使用两种步长的凸组合 次梯度法 迭代格式：$x^{k&#43;1} = x^k − η^kg^k, g^k ∈ ∂f(x^k)$ 若 $0 \notin ∂f(x)$，那么对于任意 $x^∗ ∈ argmin_x f(x)$和任意 $g ∈ ∂f(x)$，存在步长 $η &gt; 0$ 使得$||x−ηg−x^||_2^2 &lt;||x−x^||_2^2$ 若至少存在一个极小点且次梯度有界，则$\sum \eta_k(f(x^k)-f(x^))\le \frac{1}{2}||x^0-x^||^2&#43;\frac{1}{2}\sum \eta_k^2 M^2$ 经典牛顿法 迭代格式：$x^{k&#43;1} = x^k − \nabla^2f(x^k)^{-1}\nabla f(x^k), g^k ∈ ∂f(x^k)$ 极小点处梯度为0，Hessian矩阵正定，则起始点足够近时，收敛是Q-二次的且梯度的范数Q-二次收敛到0 修正牛顿法 迭代格式：$x^{k&#43;1} = x^k &#43;\eta_k d^k$ 确定矩阵$E^k$使得$\nabla ^2 f(x^k)&#43;E^k$正定且条件数较小，求解$B^kd^k=-\nabla f(x^k)$，确定步长迭代。 非精确牛顿法 引入残差$r^k=\nabla^2 f(x^k)d^k&#43;\nabla f(x^k)$，$||r^k||\le \alpha_k||\nabla f(x^k)||$ 若存在$t&lt;1$使得$0&lt;\alpha_k&lt;t$则Q-线性收敛；若$\alpha_k$收敛到0，则Q-超线性收敛；若$\alpha_k=O(||\nabla f(x^k)||)$，则Q-二次收敛 拟牛顿条件 Hessian的近似矩阵满足$y^k=B^{k&#43;1}s^k$，逆矩阵$s^k=H^{k&#43;1}y^k$ 迭代格式：$x^{k&#43;1}=x^k&#43;\alpha_k d^k$，$d^k=-(B^k)^{-1}\nabla f(x^k)=-H^k\nabla f(x^k)$ SR1秩一更新 $B^{k&#43;1}=B^k&#43;\frac{(y^k-B^ks^k)(y^k-B^ks^k)^T}{(y^k-B^ks^k)^T s^k}$ $H^{k&#43;1}=H^k&#43;\frac{(s^k-H^ky^k)(s^k-H^ky^k)^T}{(s^k-H^ky^k)^T y^k}$ 秩二更新 BFGS(相当于在满足割线方程的对称矩阵中找到离 $H^k$ 最近的矩阵) 利用割线方程$Ws^k=y^k$ $B^{k&#43;1}=B^k&#43;\frac{y^k(y^k)^T}{(s^k)^T y^k}-\frac{B^k s^k(B^ks^k)^T}{(s^k)^T B^ks^k}$ $H^{k&#43;1}=(I-\rho_k y^k(s^k)^T)^TH^{k}(I-\rho_k y^k(s^k)^T)&#43;\rho_ks^k(s^k)^T, \rho=\frac{1}{s^T y}$ DFP方法，和BFGS为对偶关系 $Wy^k=s^k$ 收敛性质 Zoutendijk 条件：满足Wolfe准则的一般迭代格式，有下界、连续可微、梯度利普希茨连续，则$\sum_{k=0}^\infty \cos^2(\theta_k)||\nabla f(x^k)||^2&lt;\infty$，$\cos\theta_k=\frac{-\nabla f(x^k)^T d^k}{||\nabla f(x^k)^T ||||d^k||}$ BFGS 全局收敛性：初始矩阵$B^0$对称正定，目标函数连续可微，对$f(x^0)$下水平集凸，且存在正数$m$以及$M$对任意$x,z$有$m||z||^2\le z^T \nabla ^2 f(x)z \le M||z||^2$，则 BFGS 格式结合 Wolfe 线搜索的拟牛顿算法全局收敛到极小值点 BFGS 收敛速度：目标二阶连续可微，最优点邻域Hessian矩阵利普希茨连续，BFGS收敛，误差之和小于正无穷，则Q-超线性收敛 约束优化最优性理论 拉格朗日函数$L(x,\lambda,\nu)=f(x)&#43;\sum_{i\in I} \lambda_i c_i(x)&#43;\sum_{i \in E} \nu_i c_i(x)$ 对偶函数$g(\lambda, \nu)=\inf_x L(x,\lambda,\nu)$是凸函数，给出原优化问题的下界$g(\lambda,\nu)\le p^*$ 最优下界$\max g(\lambda,\nu)=max_{\lambda\ge 0,v}\inf_x L(x,\lambda,\nu)$ $domg = {(λ,ν) | λ ≥ 0,g(λ,ν) &gt; −∞}$，当 $(λ, ν) ∈ \mathrm{dom} g$ 时，称为对偶可行解，对偶问题的最优值为 $q^∗$.称 $p^∗ − q^∗(≥ 0)$ 为对偶间隙，对偶间隙为零，则强对偶原理成立 拉格朗日函数不动点$\nabla_x L(x^,\lambda_1^)=0$是必需但不充分的 某点$x^$不存在一阶可行下降方向时，$\nabla_x L(x^,\lambda_1^)=0,\lambda_1^\ge 0$且(互补松弛条件：)$\lambda_1^c_1(x^)=0$ 切锥$T_X(x)$：切向量$d=\lim_{k\rightarrow \infty}\frac{z_k-x}{t_k}$的集合，最优化要求切锥(可行方向集合)不包含使得目标函数值下降的方向 几何最优性条件：对局部极小点的可行点，目标和约束函数可微，则$d^T\nabla f(x^)\ge 0, \forall d \in T_X(x^)$&lt;=&gt;$T_X(x^)\cap{d|\nabla f(x^)^T d&lt;0}=\varnothing$ 线性化可行锥：$F(x)={d|d^T∇c_i(x) = 0, ∀ i ∈ E； d^T∇c_i(x)≤0,∀i∈A(x)∩I}$，积极集$A(x)=E∪{i∈I : c_i(x)=0}$ 线性无关约束规格：给定可行点 $x$ 及相应的积极集 $A(x)$. 如果积极集对应的约束函数的梯度, 即 $∇c_i(x), i ∈ A(x)$, 是线性无关的, 则称线性无关约束规格 (LICQ) 在点 $x$ 处成立，如果LICQ 成立，则有 $T_X (x) = F (x)$ MFCQ：如果存在一个向量 $w ∈ R^n$, 使得$∇c_i(x)^Tw &lt; 0, ∀i ∈ A(x) ∩ I;∇c_i(x)^Tw = 0, ∀i ∈ E$，并且等式约束对应的梯度集 ${∇c_i(x), i ∈ E}$是线性无关的，则称 MFCQ 在点 x 处成立 KKT条件：（如果局部极小点处有$T_X (x^∗) = F (x^∗)$） 稳定性条件$\nabla_x L(x^,\lambda^)=\nabla f(x^)&#43;\sum_{i\in I\cup E} \lambda_i^\nabla c_i(x^*)=0$ 原始可行性条件 $c_i (x^∗) = 0, ∀i ∈ E,$^ 原始可行性条件 $c_i (x^∗) ⩽ 0, ∀i ∈ I$ 对偶可行性条件 $λ^∗_i ⩾0,∀i∈I$ 互补松弛条件 $λ^∗_i c_i (x^∗) = 0,∀i ∈ I$ 二阶最优性条件： 二阶必要条件：如果局部最优解处处有$T_X (x^∗) = F (x^∗)$，$(x^,\lambda^)$满足KKT条件，则$d^T∇^2_{xx}L(x^∗,λ^∗)d ⩾ 0, ∀d ∈ C (x^∗,λ^∗)$ 二阶充分条件：$d^T∇^2_{xx}L(x^∗,λ^∗)d&gt;0, ∀d∈C(x^∗,λ^∗),d\ne0$，那么 $x^∗$ 为一个严格局部极小解. 约束优化方法 二次罚函数法 等式二次罚函数 $P_E(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)，\sigma&gt;0$ 给定 σ1 &gt; 0,x0,k ← 1.罚因子增长系数 ρ &gt; 1; while 未达到收敛准则 do 以 xk 为初始点，求解 x[k&#43;1] = argmin PE (x, σk); 选取 σ[k&#43;1] = ρ*σ[k]; k ← k &#43; 1; end 收敛性： 设 $x^{k&#43;1}$ 是 $P_E (x, σ^k)$ 的全局极小解, $σ^k$ 单调上升趋于无穷, 则 $x^k$ 的每个极限点$x^∗$都是原问题的全局极小解 $\sigma c_i\rightarrow -\lambda_i^*$（一定条件下） 不等式二次罚函数 $P_I(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma \sum_{i\in I}\tilde{c}_i^2(x)，\sigma&gt;0, \tilde{c}_i(x)=\max{c_i(x),0}$ 一般约束的二次罚函数 $P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i^2(x)&#43;\sum{i\in E}c_i^2(x))$ 内点罚函数（常用对数） $P_I(x,\sigma)=f(x)-\sigma \sum_{i\in I}\ln(-c_i(x))$( 罚因子逐渐缩小，系数$\rho$) 收敛性：$|\sigma_k\sum_{i \in I}(-c_i(x^{k&#43;1})|\le \varepsilon$（实际上极限为0） 精确罚函数法 $l_1$罚函数：$P(x,\sigma)=f(x)&#43;\frac{1}{2}\sigma (\sum_{i\in I}\tilde{c}i(x)&#43;\sum{i\in E}|c_i(x)|)$ 当罚因子充分大 $σ&gt;||λ^*||_∞$(不需要是正无穷) 时，原问题的极小值点就是罚函数的极小值点 增广拉格朗日函数法 等式约束 增广拉格朗日函数$L_\sigma(x,\lambda)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43;\frac{1}{2}\sigma \sum_{i\in E}c_i^2(x)$ 初始坐标、乘子、罚因子及其更新常数，约束违反常数，精度，迭代步数 for k=.. do 从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度 if 等式约束满足精度 then 返回近似解，终止 else 更新乘子、罚因子 end 罚因子更新$σ_{k&#43;1} = ρσ{k}$，乘子更新$λ^{k&#43;1}_i=λ^k_i&#43;σ_i c_i (x^{k&#43;1})$ 一般约束约束 引入松弛变量，$L(x,s,\lambda,\mu)=f(x)&#43;\sum_{i\in E}\lambda_i c_i(x)&#43; \sum_{i\in I}\mu_i (c_i(x)&#43;s_i)$，$s_i\ge 0$；$p(x,s)=\sum_{i\in E}c_i^2(x)&#43;\sum_{i \in I}(c_i(x)&#43;s_i)^2$ 增广拉格朗日函数：$L_\sigma (x,s,\lambda,\mu)=L&#43;p(x,s)$ 取最优的$s_i=\max{-\frac{\mu_i}{\sigma_k}-c_i(x),0}$，原问题等价于优化$L_\sigma (x,\lambda,\mu)$ 初始坐标、乘子、罚因子及其更新常数，约束违反常数e，精度，常数alpha和beta、迭代步数 for k=.. do 从初始点求解增广拉格朗日函数最小值解，精度条件：梯度范数小于精度 if 约束违反度小与ek then if 约束违反度小于违反度常数e 且梯度范数小于精度 then 返回近似解，终止 else 更新两个乘子、罚因子不变，减小精度条件和约束违反度 else 乘子不变，更新罚因子，调整误差和约束违反度 end 乘子更新$E:\lambda_i^{k&#43;1}=\lambda_i^k \sigma_k c_i(x^{k&#43;1})$，$I:\mu_i^{k&#43;1}=\max{\mu_i^k &#43;\sigma_k c_i(x^{k&#43;1}),0}$ 误差和约束违反度：$\eta_{k&#43;1}=\frac{\eta_k}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{\varepsilon_k}{\sigma_{k&#43;1}^\beta}$或$\eta_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}}，\varepsilon_{k&#43;1}=\frac{1}{\sigma_{k&#43;1}^\alpha}$ 凸优化问题 交替方向乘子法ADMM 对于优化$f_1(x)&#43;f_2(x), A_1x_1&#43;A_2x_2=b$， 增广拉格朗日函数$L_\rho(x_1,x_2,y)=f_1(x_1)&#43;f_2(x_2)&#43;y^T(A_1x_1&#43;A_2x_2-b)&#43;\frac{\rho}{2}||A_1x_1&#43;A_2x_2-b||^2_2$， [[乘子更新]]$y^{k&#43;1}=y^k&#43;\tau\rho (A_1x_1^{k&#43;1}&#43;A_2x_2^{k&#43;1}-b)$ 交替求极小：$x_1^{k&#43;1}=\argmin L_\rho (x_1,x_2^k,y^k)$；$x_2^{k&#43;1}=\argmin L_\rho (x_1^{k&#43;1},x_2,y^k)$；[[乘子更新]] 随机一阶优化方法 随机梯度类算法 随机梯度法 迭代格式$x^{k&#43;1}=x^k-\eta_k \nabla f_{ik}(x^k)$ 小批量随机梯度法 迭代格式$x^{k&#43;1}=x^k-\eta\nabla f _{S_k}(x^k), \nabla f {S_k}(x^k)=\frac{1}{|S_k|}\sum{i\in S_k }\nabla f_i(x^k)$ 随机动量法 迭代格式：$v^k=\beta_k v^{k-1}&#43;\nabla f_{i_k}(x^k), x^{k&#43;1}=x^k-\eta_k v^k$ 等价于重球法$x^{k&#43;1}=x^k-\eta_k \nabla f_{i_k}(x^k)&#43;\hat{\beta}_k(x^k-x^{k-1})$ 随机次梯度法 迭代格式： $x^{k&#43;1} = x^k − η_kg^k, g^k ∈ ∂f_{i_k} (x^k)$, 当满足$\sum \eta_k =&#43;\infty, \frac{\sum_{1\sim K-1}}{\eta_k^2}{\sum_{1\sim K-1}}{\eta_k}\rightarrow 0$时算法收敛 函数的渐近表现很脆弱，这种算法结构很难实现并行化，当问题规模较大时，算法执行时间长 随机方差缩减类方法 $SGD_$：$x^{k&#43;1}=x^k-\eta(\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^))$ 动态抽样方法 范数测试、内积测试、锐角测试 分层抽样方法：将训练样本分类，每一类独立采样子集 $x^{k&#43;1}=x^k-\frac{\eta_k}{n}\sum_{i=1}^{t}\frac{n_i^k}{b_i^k}\sum_{s\in B_i^k} \nabla f_s(x^k)$ SAG算法 全梯度的估计$\bar{g}^k=\frac{1}{n}\sum_{j=1}^n v_j^k$ 迭代时$v_j^{k&#43;1}=\nabla f_{i_k}(x_k)\quad \mathrm{if} j=i_k ,\mathrm{else}: v_j^k$，即更新之后将抽取的样本对应的随机梯度改为当前的随机梯度值 由于每次只有一部分改变，可以写成$\bar{g}^k=\bar{g}^{k-1}-\frac{1}{n}v_{i_k}^{k-1}&#43;\frac{1}{n}v_{i_k}^k$ SAGA算法 SAGA 算法选择一个参考点$\bar{x}^i,v_i=\nabla f_i(\bar{x}^i)$ $g^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(\bar{x}^{i_k})&#43;\frac{1}{n}\sum_{j=1}^n \nabla f_j(\bar{x}_j)$ x 线性收敛速度 SVRG算法 每经过几次迭代之后设置检查点，计算全梯度作为参考 $\nabla f(x^j)=\frac{1}{n}\sum_{i=1}^n \nabla f_i(x^j)$ $v^k=\nabla f_{i_k}(x^k)-(\nabla f_{i_k}(x^j)-\nabla f(x^j))$ 对于参考点的函数值期望的意义下线性收敛速度 随机递归梯度法SARAH 梯度估计的更新$v^k=\nabla f_{i_k}(x^k)-\nabla f_{i_k}(x^{k-1})&#43;v^{k-1} , v^0=$全梯度 不是无偏估计 带BB步长的方差缩减类 AdaGrad $x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{G^k&#43;\varepsilon 1_n}}\circ g^k$ $G^{k&#43;1}=G^k&#43;g^k \circ g^k$ RMSProp $x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{M^k&#43;\varepsilon 1_n}}\circ g^k$ $G^{k&#43;1}=\rho G^k&#43;(1-\rho)g^{k&#43;1} \circ g^{k&#43;1}$ Adam 梯度$g^k=\nabla f_i (x^k)$ 一阶矩$S^k=\rho_1 S^{k-1}&#43;(1-\rho_1)g^k$ 二阶矩$M^k=\rho_2 M^{k-1}&#43;(1-\rho_2)g^k\circ g^k$ 一阶矩修正$\hat{S}^k=\frac{S^k}{1-\rho_1^k}$ 二阶矩修正$\hat{M}^k=\frac{M^k}{1-\rho_2^k}$ $x^{k&#43;1}=x^k-\frac{\eta}{\sqrt{\hat{M}^k&#43;\varepsilon 1_n}}\circ \hat{S}^k$ AdaBelief 修改二阶矩的计算 $Q^k=\rho_2 Q^{k-1}&#43;(1-\rho_2)(g^k-S^k)\circ (g^k-S^k)$ 修正二阶矩偏差时加入额外的$\varepsilon$保证有下界 $\hat{Q}^k=\frac{Q^k&#43;\varepsilon}{1-\rho_2^k}$ AdaBelief 算法在“大梯度，小曲率”情况下有优势 </p>
  </div>
  <footer class="entry-footer"><span title='2023-10-19 16:34:43 +0800 CST'>October 19, 2023</span>&nbsp;·&nbsp;4 min</footer>
  <a class="entry-link" aria-label="post link to 人工智能的数学基础" href="https://sjj1017.github.io/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">复杂系统决策智能
    </h2>
  </header>
  <div class="entry-content">
    <p>绪论 最优化问题 分类：变量个数、性质、约束、极值个数、目标个数、线性和非线性、确定/随机/模糊、静态/动态 函数优化问题与组合优化问题 P（多项式时间内判定或解出）、NP（多项式时间内验证）、NPC问题 计算智能方法 逻辑主义、行为主义、联结主义 神经计算、模糊计算、进化计算（遗传、粒子群）、单点搜索（模拟退火、禁忌搜索） 神经网络 基本特征：神经元及其联结、联结强度决定信号传递强弱、强度可以随着训练改变、信号可以起刺激作用或抑制作用、接收信号的累积效果决定状态、每个神经元有一个阈值 基本原理：输入层、加权和、阈值函数、输出层 单层感知器网络、前馈型网络、前馈内层互联网络、循环网（短期记忆特征，稳定，反馈信号引起的变化会减小并消失）、反馈型网络、全互联网络 学习算法： 有监督（实际输出与期望输出的偏差）和无监督（仅仅根据其输入 调整连接权系数和阈值） 梯度下降算法：一般来说，只能找到一个局部最小点(多解)，收敛速度较慢，算法结构简单 最速下降法 BP神经网络：初始化网络权值、向前传播输入、反向误差传播、网络权值调整 BRF神经网络： 求取基函数中心：网络初始化，选取聚类中心，将输入的样本按最近邻分组，计算各个聚类集合的平均值得到新的聚类中心。 求解方差 计算隐含层和输出层之间的权值 卷积神经网络 模糊逻辑 模糊集合 可以部分地属于，对于U上一个元素u， f(u)叫做u对于模糊集的隶属度，也可写作 A(u) Zadeh表示法$A=\sum \frac{f_A(u)}{u}|A=\int_u \frac{f_A(u)}{u}$ 序对表示法$A={(u,f_A(u))|u\in U}$ 子集：对任意元素都有$f_A(u)&lt;f_B(u)$则$A$是$B$的子集 交：取最小，并：取最大，补：用1减 隶属度函数呈单峰馒头形（(凸模糊集合） 模糊变量的标称值选择一般取3—9个为宜，通常取奇数 (平衡)——在“零”、“适中”或者“合适”集合的两边语言值通常取对称(如速度适中，一边取“速度高”，一般另一 边取“速度低”，满足对称)。 隶属度函数要符合人们的语义顺序，避免不恰当的重叠，在相同的论域上使用的具有语义顺序关系的若干标称的模 糊集合，应该合理的排列。下面的排列是错误的。 模糊统计法：隶属频率=属于A的次数/总次数 例证法：由已知的有限个隶属函数的值， 来估计论域U上的模糊子集A的隶属函数。 专家经验法、二元对比排序法 大概三类图形： 左大右小的偏小型下降函数(Z函数 左小右大的偏大型上升函数(S函数) 对称型凸函数(II函数) 模糊关系 $U\times V={(u,v)|u\in U,v\in V}, (u,v)\rightarrow{}R(u,v)$ 模糊矩阵 模糊关系的复合 极大-极小复合$R_1R_2={[(x,z),\max_y \min[\mu_{R1}(x,y),\mu_{R2}(y,z)]]}$($R_1,R_2$在$X\times Y$和$Y\times Z$) 极大-乘积复合$R_1R_2={[(x,z),\max_y [\mu_{R1}(x,y)*\mu_{R2}(y,z)]]}$ 模糊推理 语言变量的取值就是模糊集合。语言算子 T(年纪)={年轻，不年轻，不很年轻,…, 中年，不是中年,…,年老，非常年老,…, 不年轻也不老,….}，其中“年纪”是语言变量。 (x,T(x),X,G,M)：其中x是语言变量名;T(x)为语言变量x的语言值或语言术语集合;X为语言变量x的论域;G为产生T(x)中术语的句法规则，用于产生语言变量值的;M是赋予每 个语言值A以含义M(A)的语法规则，即隶属度函数。 模糊推理是通过模糊规则将输入转化为输出的过程。在模糊推理中，小前提没有必要与大前提的前件 一致(A与C不必完全一致)，结论没有必要与大前提的后件一致(B与D不必完全一致)。 遗传算法 模式：模式指群体中编码的某些位置具有相似结构的染色体集合，模式的阶指模式中具有确定取值的基因个数，模式的定义长度指模式中第一个具有确定取值的基因到最后一个具有确定取值的基因的距离 (把中间的空格当作距离) 染色体编码： $2^L=\frac{U_{max}-U_{min}}{\delta}&#43;1$，$U=U_{min}&#43;\frac{(U_{max}-U_{min})X}{2^L-1}$ 群体初始化 随机数初始化 适应性评价 评估函数用于评估各个染色体的适应值，进而区分优劣 选择算子 轮盘赌方法选择，选中概率与适应度大小成正比$P_i=\frac{F(x_i)}{\sum F(x_i)}$ 选择概率和积累概率 交配算子 交配概率$P_c$ 将选择出的种群中的M个个体以随机的方式组成 M/2对配对个体组，交配操作就是在这些配对个体组中的两个个体之间进行—随机配对 单点交叉（选一个交叉点，一半交叉）、多点交叉、均匀交叉（对每一个基因位随机交换或不交换） 变异算子 变异概率$P_m$ 与个体编码串长度等长的屏蔽字，确定哪些位变异 粒子群算法 初始化，随机初始化速度和位置 速度位置更新，惯量权重$\omega$，加速系数$c_i$，随机数$r_i$ $v_i=\omega v_i &#43;c_1r_1(p_{Best_i}-x_i) &#43;c_2r_2(g_{Best}-x_i)$ $x_i=x_i&#43;v_i$ 评估粒子的适应度函数值，更新粒子最优位置和全局最优位置 结束条件：gBest差值小于精度 贝叶斯网络 全概率公式、贝叶斯公式 贝叶斯网络 原因节点：没有连线以他们为终点 贝叶斯网络的预测 自顶向下的过程 把证据向量输入到贝叶斯网络B中; 对于B中的每一个没处理过的结点n，如果它具有发生的事实(证据)，则标记它为已经处理过；否则继续下面的步骤 如果它的所有父结点中有一个没有处理过，则不处理这个结点(保证自顶向下);否则，继续下面的步骤 根据结点n的所有父结点的概率以及条件概率或联合条件概 率计算结点n的概率分布，并把结点n标记为已处理 重复步骤(2)~(4)共m次。此时，结点t的概率分布就是 它的发生/不发生的概率。算法结束。 贝叶斯网络诊断 把证据向量输入到贝叶斯网络B中 对于B中的每一个没处理过的结点n，如果它具有发生的事实(证据)，则标记它为已经处理过；否则继续面的步骤 如果它的所有子结点中有一个没有处理过，则不处理这个结点(保证自底向上)；否则，继续下面的步骤 根据节点n所有子结点的概率以及条件概率或联合条件概率，根据条件概率公式，计算结点n的概率分布，并把结点n标记为已处理; 重复步骤共m次。此时，原因结点t的概率分布就是它的发生/不发生的概率。算法结束。 贝叶斯网络训练 在两个结点之间建立连线时，要防止环的出现，因为贝叶斯网络必须是无环图 通过历史数据获得贝叶斯网络中各结点的概率以及结点之间条件概率的过程 STATIC OPTIMIZATION 无约束优化问题 $L=\frac{1}{2}u^T Qu&#43;S^T u$=&gt;$u*=-Q^{-1}S$ 等式约束优化 方法一： $dL=L_u^T du&#43;L_x^T dx$,$df=f_udu&#43;f_xdx$ $dL/du=L_u-f_u^Tf_x^{-T}L_x=0$ 方法二： \[ \begin{bmatrix} dL\df \end{bmatrix} =\begin{bmatrix} L_x^T&amp;L_u^T\f_x&amp;f_u \end{bmatrix}\begin{bmatrix} dx\du \end{bmatrix}=0 \] \[\begin{bmatrix} 1&amp;\lambda^T \end{bmatrix} \begin{bmatrix} L_x^T&amp;L_u^T\f_x&amp;f_u \end{bmatrix}=0, \quad \frac{\partial L}{\partial f}|_{du=0}-\lambda \] 方法三： $H(x,u,\lambda)=L(x,u)_\lambda^Tf(x,u)$ $H_u=H_x=H_\lambda=0$ Effect of Changes in Constraints 约束改变$\mathrm{d}f$，则$\mathrm{d}x=f_x^{-1}(I&#43;f_u C)\mathrm{d}f$，$\mathrm{d}u=-(L_{uu}^{f})^{-1}(H_{ux}-f^T_u f^{-T}x H{xx})f_x^{-1}\mathrm{d}f$ 约束优化： 初始点$u$ 确定$x$: $f(x,u)=0$ 确定乘子$\lambda=-f_x^{-T}L_x$ 确定梯度$H_u=L_u&#43;f_u^T\lambda$ 更新$\Delta u=-\alpha H_u$ 计算$\Delta L=H_u^T \Delta u$，根据要求回第二步。 动态规划方法 难点：离散化模型面临维数灾难、HJB 方程一般难以求解、HJB 方程对值函数有可微的要求 最优性原理：多级决策过程的最优策具有如下性质:不论初始状态和初始决策如何，其余的决策对于由初始决策所形成的状态来说，必定也是一个最优策略。 动态规划求解最短路径，从终点开始向后求解。 动态规划求解离散最优控制 离散化时间：$t_k\in [t_0&#43;k\Delta t,t_0&#43;(k&#43;1)\Delta t]$ 离散化状态方程$：\dot{x}(t)=f(x(t),u(t),t),x(t_0)=x_0$; $x(k&#43;1)=f_D(x(k),u(k),k)$ 离散化性能指标：$J=h_D(x(N),N)&#43;\sum_{k=0}^{N-1}g_D(x(k),u(k),k)$ Bellman 方程 最优控制下的性能 $V(x_0,k_0)$，Bellman方程是充要条件：$V(x(k),k)=min_{u(k)\in U}{g_D(x(k),u(k),k)&#43;V(x(k&#43;1),k&#43;1) }$, $V(x(N),N)=h_D(x(N),N)$ 直接迭代求解 从后往前依次求解$u(k)$ 遍历离散状态和离散控制空间 将$x(k)$离散化为$x^0,…,x^{s-1}$，$u(k)=u^0,…,u^{c-1}$ $V(x(k), k) = min {g_D(x(k), u(k), k) &#43; V(x(k &#43; 1), k &#43; 1)}$ 查表：直接寻找距离最近的 插值计算：不直接查表，使用插值近似 遍历当前和下时刻离散状态空间 每个时刻求解析解，只需要遍历离散化的状态$x(k),x(k&#43;1)$ 连续$J(u,x_0,t_0)=h(x(x_f),t_f)&#43;\int_{t_0}^{t_f} g(x(t),u(t),t)\mathrm{d}t$ 最优控制充要条件HJB方程：$-V_t(x(t),t)=\min H(x(t),u(t),V^T_x(x(t),t),t)$=&gt;$V_t&#43;\min_u{V_x^T \dot{x} &#43;g(x,u,t) }=0$ 边界条件$V(x(t_f),t_f)=h(x(t_f),t_f)$ 没有终值可以增加一个罚函数项 值函数不可微的情况：分片考虑依然满足HJB方程，可验证是充分条件. 自适应动态规划 无限域最优控制问题动态规划，无法从终点开始 自适应动态规划方法由三个网络组成：模型网络、评判网络、执行网络 $x(k)$-&gt;Action Network-&gt;$u(k)$; $u(k),x(k)$-&gt;Model Network-&gt;$x(k&#43;1)$; $x(k&#43;1)$-&gt;Critic Network-&gt;$J(x(k&#43;1))$; </p>
  </div>
  <footer class="entry-footer"><span title='2023-09-30 16:34:43 +0800 CST'>September 30, 2023</span>&nbsp;·&nbsp;2 min</footer>
  <a class="entry-link" aria-label="post link to 复杂系统决策智能" href="https://sjj1017.github.io/posts/%E5%A4%8D%E6%9D%82%E7%B3%BB%E7%BB%9F%E5%86%B3%E7%AD%96%E6%99%BA%E8%83%BD/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://sjj1017.github.io/">Jiajun, Shen</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
